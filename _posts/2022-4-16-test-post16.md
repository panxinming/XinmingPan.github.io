---
layout: post
title:  Convolutional Neural Network 
---

In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of artificial neural network, most commonly applied to analyze visual imagery. It has applications in image and video recognition, recommender systems, image classification, image segmentation, medical image analysis, natural language processing and so on.

The image classification problem is the problem of assigning a label to an image. For example, we might want to assign the label "duck" to pictures of ducks, the label "frog" to pictures of frogs, and so on. 

Here, I will introduce some of the most important tools for image classification: **convolutional neural networks**.

```python
import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt
import numpy as np
```


## Getting Data

we'll use a subset of the [CIFAR-10 data set](https://www.cs.toronto.edu/~kriz/cifar.html). This data set can be conveniently accessed using a method from `tensorflow.keras.datasets`:

```python
(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()
# Normalize pixel values to be between 0 and 1
train_images, test_images = train_images / 255.0, test_images / 255.0
```

There are 50,000 training images and 10,000 test images. Each image has **32x32** pixels, and there are three color "channels" -- **red**, **green**, and **blue**.

```python
train_images.shape, test_images.shape
```
```
((50000, 32, 32, 3), (10000, 32, 32, 3))
```

There are **10** classes of image, encoded by the labels arrays.

<br>

```python
train_labels[0:5]
```
```
array([[6],
       [9],
       [9],
       [4],
       [1]], dtype=uint8)
```

Each class corresponds to a type of object, so let's give them names.

```python
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']
```

```python
plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images[i])
    # The CIFAR labels happen to be arrays, 
    # which is why you need the extra index
    plt.xlabel(class_names[train_labels[i][0]])
plt.show()
```

![]({{ site.baseurl }}/images/cnn1.png)



## Convolution

Convolution is a mathematical operation commonly used to extract **features** (meaningful properties) from images. The idea of image convolution is pretty simple. We define a **kernel** matrix containing some numbers, and we "slide it over" the input data. At each location, we multiply the data values by the kernel matrix values, and add them together. Here's an illustrative diagram:

![]({{ site.baseurl }}/images/cnn2.jpg)

<br>

![](https://d2l.ai/_images/correlation.svg)

The value of 19 in the output is obtained in this example by computing $$0 \times 0 + 1 \times 1 + 3 \times 2 + 4 \times 3 = 19$$. 

This operation might seem either abstract or trivial, but it can be used to extract useful image features. For example, let's manually define a kernel and use it to perform "edge detection" in a greyscale image. 


```python
im = train_images[9,:,:,0] 
# this one's a cat, only taking the "blue" channel for convenience. Recall that we have blue, red and green channel.
plt.imshow(im, cmap = "gray")
plt.show()
```
![]({{ site.baseurl }}/images/cnn3.png)

<br>

Let's create a random kernel.
```python
kernel = np.array([[-1, -1, -1], 
                   [-1,  8, -1], 
                   [-1, -1, -1]])
```


Convolutional Layer is to figure out the boundary of one image. Observe that the convolved image (right) has darker patches corresponding to the distinct "edges" in the image, where darker colors meet lighter colors. 
```python
from scipy.signal import convolve2d
# Compute the gradient of an image by 2D convolution
convd = convolve2d(im, kernel, mode = "same")
fig, axarr = plt.subplots(1, 2)
axarr[0].imshow(im, cmap = "gray")
axarr[1].imshow(convd, cmap = "gray", vmin = 0, vmax = 1)
plt.show()
```

![]({{ site.baseurl }}/images/cnn4.png)


## Learning Kernels

There are lots of convolutional kernels we could potentially use. How do we know which ones are meaningful? In practice, we don't. So, we treat them as parameters, and learn them from data as part of the model fitting process. This is exactly what the `Conv2d` layer allows us to do. 


```python
# create a layer which has 32 numbers of output
# 3 \times 3 size kernal
# And the Activation function to use, we usually use relu function.
conv = layers.Conv2D(32, (3, 3), 
                     activation='relu', 
                     input_shape=(32, 32, 3),
                     dtype = "float64")
```

```python
# pick an individual image while preserving dimensions
color_im = train_images[9:10]

# perform convolution and extract as numpy array
# Now our image after convolution has 32 features, because in last step we define our output as 32.
convd = conv(color_im).numpy()

# get a single feature (corresponding to one choice of convolution)
feature = convd[0,:,:,0]

plt.imshow(feature, cmap = "gray")
plt.gca().axis("off")
```

![]({{ site.baseurl }}/images/cnn5.png)



Let's compare a few other possibilities:
```python
fig, axarr = plt.subplots(3, 3, figsize = (8, 6))

axarr[0, 0].imshow(color_im[0])
axarr[0,0].axis("off")
axarr[0,0].set(title = "Original")

i = 0
# compare 9 features to see which is the best.
for ax in axarr.flatten()[1:]:
    ax.imshow(convd[0,:,:,i], cmap = "gray")
    i += 1
    ax.axis("off")
    ax.set(title = "Feature " + str(i))
    
plt.tight_layout()
```

![]({{ site.baseurl }}/images/cnn6.png)

These features may or may not be informative -- they are purely random! We can try to learn informative features by embedding these kernels in a model and optimizing. 


## Building a Model

The most common approach is to alternate `Conv2D` layers with `MaxPooling2D` layers. Pooling layers act as "summaries" that reduce the size of the data at each step. After we're done doing "2D stuff" to the data, we then need to `Flatten` the data from 2d to 1d in order to pass it through the final `Dense` layers, which form the prediction. 

```python
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10) # number of classes
])
```



What does max pooling do? you can think of it as a kind of "summarization" step in which we intentionally make the current output somewhat "blockier." Technically, it involves sliding a window over the current batch of data and picking only the largest element within that window. Here's an example of how this looks: 

![](https://computersciencewiki.org/images/8/8a/MaxpoolSample2.png)

*Image credit: Computer Science Wiki*

Ok, now that we know what each of our layers are doing, we can now inspect our model. 


```python
model.summary()
```
```
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 30, 30, 32)        896       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 13, 13, 32)        9248      
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 6, 6, 32)          0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 4, 4, 64)          18496     
_________________________________________________________________
flatten (Flatten)            (None, 1024)              0         
_________________________________________________________________
dense (Dense)                (None, 64)                65600     
_________________________________________________________________
dense_1 (Dense)              (None, 10)                650       
=================================================================
Total params: 94,890
Trainable params: 94,890
Non-trainable params: 0
```


Let's train our model and see how it does! 


```python
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model.fit(train_images, 
                    train_labels, 
                    epochs=10, 
                    steps_per_epoch = 100,
                    validation_data=(test_images, test_labels))
```
```
Epoch 1/10
100/100 [==============================] - 24s 216ms/step - loss: 1.8661 - accuracy: 0.3209 - val_loss: 1.6024 - val_accuracy: 0.4139
Epoch 2/10
100/100 [==============================] - 14s 144ms/step - loss: 1.5187 - accuracy: 0.4482 - val_loss: 1.4325 - val_accuracy: 0.4810
Epoch 3/10
100/100 [==============================] - 15s 145ms/step - loss: 1.4012 - accuracy: 0.4950 - val_loss: 1.3481 - val_accuracy: 0.5146
Epoch 4/10
100/100 [==============================] - 14s 140ms/step - loss: 1.3205 - accuracy: 0.5265 - val_loss: 1.2855 - val_accuracy: 0.5411
Epoch 5/10
100/100 [==============================] - 14s 141ms/step - loss: 1.2687 - accuracy: 0.5509 - val_loss: 1.2335 - val_accuracy: 0.5584
Epoch 6/10
100/100 [==============================] - 14s 142ms/step - loss: 1.2075 - accuracy: 0.5731 - val_loss: 1.2299 - val_accuracy: 0.5667
Epoch 7/10
100/100 [==============================] - 14s 144ms/step - loss: 1.1798 - accuracy: 0.5829 - val_loss: 1.1505 - val_accuracy: 0.5948
Epoch 8/10
100/100 [==============================] - 14s 145ms/step - loss: 1.1322 - accuracy: 0.6021 - val_loss: 1.1454 - val_accuracy: 0.5968
Epoch 9/10
100/100 [==============================] - 15s 152ms/step - loss: 1.1041 - accuracy: 0.6131 - val_loss: 1.1065 - val_accuracy: 0.6135
Epoch 10/10
100/100 [==============================] - 14s 145ms/step - loss: 1.0665 - accuracy: 0.6275 - val_loss: 1.0779 - val_accuracy: 0.6228
```

<br>

```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```
![]({{ site.baseurl }}/images/cnn7.png)


After just a few rounds of training, our model is able to guess the image label more than 50% of the time on the unseen validation data, which is relatively impressive considering that there are 10 possibilities. 

Note: the training process can often be considerably accelerated by training on a GPU. A limited amount of free GPU power is available via Google Colab, and is illustrated [here](https://colab.research.google.com/notebooks/gpu.ipynb). 



## Extracting Predictions

Let's see how our model did on the test data: 

```python
y_pred = model.predict(test_images)
labels_pred = y_pred.argmax(axis = 1)
```


We'll plot these predicted labels along side the (true labels).

```python
plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(test_images[i])
    plt.xlabel(class_names[labels_pred[i]] + f" ({class_names[test_labels[i][0]]})")
plt.show()
```
![]({{ site.baseurl }}/images/cnn8.png)


Overall, these results look fairly reasonable. There are plenty of mistakes, but it does look like the places where the model made errors are authentically somewhat confusing. A more complex or powerful model would potentially be able to do noticeably better on this data set. 


# Visualizing Learned Features 

It's possible to define a separate model that allows us to study the features learned by the model. These are often called *activations*. We create this model by simply asserting that the model outputs are equal to the outputs of the first convolutional layer. For this we use the `models.Model` class rather than the `models.Sequential` class, which is more convenient but less flexible. 

It's possible to look at the activations at different levels of the model. Generally speaking, it is expected that the activations become more abstract as one goes higher up the model structure. 

```python
activation_model = models.Model(inputs=model.input, 
                                outputs=model.layers[0].output)
```

Now we can compute the activations

```python
activations = activation_model.predict(train_images[0:10]) 
k = 7

color_im = train_images[k:(k+1)]
convd = conv(color_im).numpy()

fig, axarr = plt.subplots(3, 3, figsize = (8, 6))

axarr[0, 0].imshow(color_im[0])
axarr[0,0].axis("off")
axarr[0,0].set(title = "Original")

i = 0
for ax in axarr.flatten()[1:]:
    ax.imshow(activations[k,:,:,i], cmap = "gray")
    i += 1
    ax.axis("off")
    ax.set(title = "Feature " + str(i))
    
plt.tight_layout()
```
![]({{ site.baseurl }}/images/cnn9.png)

Somewhat romantically, these activations might be interpreted as "how the algorithm looks at" the resulting image. That said, one must be careful of over-interpretation. Still, it looks like some of the features correspond to edge detection (like we saw above), while others correspond to highlighting different patches of colors, enabling, for example, separation of the foreground object from the background. 