---
layout: post
title:  TensorFlow 2.0
---


[TensorFlow](https://www.tensorflow.org/) is a free and open-source software library for machine learning and artificial intelligence. It can be used across a range of tasks but has a particular focus on training and inference of deep neural networks. 

Here, I want to introduce what is **TensorFlow** and What is **Neural Networks** in details to help you understand clearly. And I will use the TensorFlow package for advanced machine learning, with an emphasis on neural networks.


## A). TensorFlow

Before we get in touch with TensorFlow, we must know what is Tensor.


### I. Tensors

> So, uh, what's a tensor? 

In simple words, A tensor is a container which can hold data in $$N$$ dimensions. Tensors are generalizations of matrices to $$N$$-dimensional space. For example, when $$N=1$$, a tensor is a scalar. When $$N=2$$, a tensor is a vector.  

![tf1.jpg]({{ site.baseurl }}/images/tf1.png)


<br>

In Python, A tensor is pretty much just a **Numpy** array. 

Most of the time, we'll work with 2-dimensional tensors (matrices), with the occasional 3-dimensional tensor thrown in for spice.



### II. Structure of Neural Networks

Before we start working with **TensorFlow**, I'd like to build up our understanding of what a neural network actually does to data. We'll see that neural networks are built up of very simple mathematical transformations, which are then stacked on top of each other to produce complex models. 

Suppose I have a data point $$x$$, with 5 features (columns): 

$$X_i = \begin{bmatrix}
x_1 & x_2 & x_3 & x_4 & x_5
\end{bmatrix}$$

A single *unit* of a neural network has two stages: 

1. The unit takes the entries of $$x$$, multiplies them by some *weights*, and adds them together, forming the linear combination $$y = \sum_i w_i x_i$$. This number $$y$$ is often called the *activation*. The number $$w_i$$ is also known as weight.
2. Second, the unit hits $$y$$ with a non-linear function, producing a new output $$z$$: $$z = g(y)$$.


A common nonlinear function is the sigmoid (same function as used in logistic regression): The higher score $$z$$ we have, that means our model is good.



By given a weight vector $$w$$, we can easily compute the layer output  for $$x$$, remembering that `w @ x` will calculate the inner product for us. 

In practice, we perform this computation using many data points and many units simultaneously. In this case, we can represent the data points as an $$m \times n$$ matrix $$\mathbf{X}$$, where $$m$$ is the number of data points and $$n$$ is the number of columns. We can represent the matrix of weights as an $$p \times n$$ matrix $$\mathbf{W}$$, where $$p$$ is the number of units in the layer. We can then compute all the activations simultaneously by computing the matrix product $$\mathbf{Y} = \mathbf{W} \mathbf{X}^T$$. 

![](https://www.jeremyjordan.me/content/images/2018/01/Screen-Shot-2017-11-07-at-12.53.07-PM.png)

### III. Example in Neural Networks

Suppose we are going to do the image classification by using Neural Networks. We can treat every image as a tensor (As we mentioned above). One image has 3-dimentional tensor. So, it has 
$$ inputSize \times inputSize \times 3$$ entires inside. 

For example, we have one image which has $$32 \times 32 \times 3$$ size. So, it has $$3072$$ entries in total. If this image represents dog. So, we can treat our one observation as 

$$X_i = \begin{bmatrix}
x_1 & x_2 & x_3 & ... & x_{3072}
\end{bmatrix}$$

 And our weight vector as 

 $$W_i = \begin{bmatrix}
w_1 & w_2 & w_3 & ... & w_{3072}
\end{bmatrix}$$

Then,

$$\mathbf{W} \mathbf{X}^T + \mathbf{b} = a$$

The $$a$$ is called activation. After we got our activation, we need to compare our activation by different weighted vector. Suppose we have weighted vector $$W_1 = \begin{bmatrix}
w_{1,1} & w_{1,2} & w_{1,3} & ... & w_{1,3072}
\end{bmatrix}$$ represent dog, $$W_2 = \begin{bmatrix}
w_{2,1} & w_{2,2} & w_{2,3} & ... & w_{2,3072}
\end{bmatrix}$$ represent dog and $$W_3 = \begin{bmatrix}
w_{3,1} & w_{3,2} & w_{3,3} & ... & w_{3,3072}
\end{bmatrix}$$ represent car. So, we have have 

$$\begin{bmatrix}
w_{1,1} & w_{1,2} & w_{1,3} & ... & w_{1,3072} \\
w_{2,1} & w_{2,2} & w_{2,3} & ... & w_{2,3072} \\
w_{3,1} & w_{3,2} & w_{3,3} & ... & w_{3,3072}
\end{bmatrix} \begin{bmatrix}
x_1 \\ x_2 \\ x_3 \\ ... \\ x_{3072}
\end{bmatrix} + \begin{bmatrix}
b_1 \\ b_2 \\ b_3
\end{bmatrix}= \begin{bmatrix}
a_1 \\ a_2 \\ a_3
\end{bmatrix}$$

If $$a_1$$ has the highest activation, so can treat our image as dog. However, as we mentioned above, we need to hits the activation into non-linear function to get a new output $$z$$: $$z = g(y)$$. Most of the Activation Functions can be found in this [link](https://en.wikipedia.org/wiki/Activation_function).

And we also have a function called [loss function](https://en.wikipedia.org/wiki/Loss_function) which can help us determine which image it comes from.

## b). TensorFlow Basics

We can install TensorFlow in our command prompt under the environment Anaconda. Type:
```
pip install tensorflow
```

After we installed, let's check the version.

```python
import tensorflow as tf
tf.__version__
```
```
'2.8.0'
```

<br>

We can create a simple, "constant" tensor using `tf.constant`: 

```python
x = tf.constant([[1,9], [3,6]])
x
```
```
<tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[1, 9],
       [3, 6]])>
```


<br>

And we can check it's shape:

```python
x.shape
```
```
TensorShape([2, 2])
```

<br>

As you can see from the output, this object has a `shape`, a `dtype`, and even an internal `numpy` representation -- it really is a lot like a `numpy` array! Like `numpy` arrays, we can do various mathematical operations: 

```python
x = tf.add(x,1)
x
```
```
<tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[ 2, 10],
       [ 4,  7]])>
```

```python
x1 = tf.ones([2,2])
x2 = tf.multiply(x1, 2)
x2
```
```
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[2., 2.],
       [2., 2.]], dtype=float32)>
```

<br>

There *is* a reason that we use `tf.Tensor` rather than Numpy array objects. One of the primary reasons is that the tensor data type is set up to support *automatic differentiation*, which is important when it comes time to train our models. That said, it's sometimes useful to convert back to literal Numpy arrays, which can usually be done like this: 

```python
x.numpy()
```
```
array([[ 2, 10],
       [ 4,  7]])
```

<br>

Because computation on 64-bit floating point numbers can be quite expensive at scale, most operations in TensorFlow prefer that you supply floating point numbers with data type `float32`. For example: 

```python
x = tf.cast(x, tf.float32)
x
```
```
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[ 2., 10.],
       [ 4.,  7.]], dtype=float32)>
```

<br>

Supplying `float64` data types will usually lead to annoying warning messages, and possibly slower performance. 

While it's good to know what tensors are and how they work, we usually won't need to construct them explicitly. Rather, we'll be able to feed Numpy arrays to our models, which will handle all the tensor operations internally.

## c). Layers

*Layers* are the building blocks of models. You can think of a layer as a function that takes in one tensor and spits out another tensor, possibly of a different shape. Many layers have requirements on the kinds of tensors they admit; for example, they might only work on 2d tensors. TensorFlow in Python already provide us many layers, like convolutional layers, dense layers etc.


![tf1.jpg]({{ site.baseurl }}/images/tf2.png)

We can create our Neural Networks models by doing this
```python
from tensorflow.keras import layers
import tensorflow.keras
model = tf.keras.Sequential()
```

And we can add hidden layers inside our model. 

```python
model.add(layers.Dense(16))
model.add(layers.Dense(32))
model.add(layers.Dense(1))
```

**Dense** Layer is simple layer of neurons in which each neuron receives input from all the neurons of previous layer, thus called as dense. Dense Layer is used to classify image based on output from convolutional layers.

$$y = \sum_i w_i x_i + b$$


## c). Neural Networks in Regression

Neural Networks can deal with regression problems, so let's take a look of one simple example. We are going to use the iris data again.

```python
import pandas as pd
from matplotlib import pyplot as plt
import numpy as np
iris = pd.read_csv("iris.csv")

plt.scatter(iris["Sepal.Length"], iris["Petal.Length"])
plt.show()
```

![tf3.jpg]({{ site.baseurl }}/images/tf3.png)

Here, we are going to use Sepal.Length to predict the Petal.Length. So, we can let Petal.Length become $$y$$.

```python
x = iris["Sepal.Length"]
y = iris["Petal.Length"]
```

Then, create our Neural Network Model.

```python
model = tf.keras.Sequential()
model.add(layers.Dense(16))
model.add(layers.Dense(32))
model.add(layers.Dense(1))
# choose our optimizer, here we choose a tpycial one.
model.compile(optimizer = tf.keras.optimizers.SGD(0.001),
             loss = "mean_squared_error")
# fit our model by switching our model into 20% test
model.fit(x,y,validation_split = 0.2, epochs=10, batch_size = 64)
```
```
Epoch 1/10
2/2 [==============================] - 0s 39ms/step - loss: 1.6637 - val_loss: 1.8540
Epoch 2/10
2/2 [==============================] - 0s 17ms/step - loss: 1.6596 - val_loss: 1.9208
Epoch 3/10
2/2 [==============================] - 0s 19ms/step - loss: 1.6607 - val_loss: 2.1139
Epoch 4/10
2/2 [==============================] - 0s 28ms/step - loss: 1.6739 - val_loss: 2.2256
Epoch 5/10
2/2 [==============================] - 0s 19ms/step - loss: 1.6575 - val_loss: 1.9818
Epoch 6/10
2/2 [==============================] - 0s 20ms/step - loss: 1.6562 - val_loss: 2.0697
Epoch 7/10
2/2 [==============================] - 0s 19ms/step - loss: 1.7332 - val_loss: 2.5217
Epoch 8/10
2/2 [==============================] - 0s 16ms/step - loss: 1.6702 - val_loss: 2.0808
Epoch 9/10
2/2 [==============================] - 0s 20ms/step - loss: 1.6583 - val_loss: 2.1160
Epoch 10/10
2/2 [==============================] - 0s 18ms/step - loss: 1.6526 - val_loss: 1.9143
```

Let's see what's our model looks like.
```python
model.summary()
```
```
Model: "sequential_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 16)                32        
                                                                 
 dense_1 (Dense)             (None, 32)                544       
                                                                 
 dense_2 (Dense)             (None, 1)                 33        
                                                                 
=================================================================
Total params: 609
Trainable params: 609
Non-trainable params: 0
_________________________________________________________________
```

How these parameters comes. The first layer has 32 parameters. Because we have one feature, so the weight matrix must have 16 neurons, and the length of our first output must have the same length as before, so the bias also has $$16 \times 1$$ dimension. So, the first layer have 32 paramters.


$$\begin{bmatrix}
w_{1,1} & w_{1,2} & w_{1,3} & ... & w_{1,i} \\
w_{2,1} & w_{2,2} & w_{2,3} & ... & w_{2,i} \\
...\\
w_{16,1} & w_{16,2} & w_{16,3} & ... & w_{1,i} \\
\end{bmatrix} \begin{bmatrix}
x_1 \\ x_2 \\ x_3 \\ ... \\ x_{i}
\end{bmatrix} + \begin{bmatrix}
b_1 \\ b_2 \\ b_3 \\ ... \\ b_{16}
\end{bmatrix}= \begin{bmatrix}
a_1 \\ a_2 \\ a_3 \\ ... \\ a_{16}
\end{bmatrix}$$


$$parameters_1 = 1 \times 16 + 16$$

$$parameters_2 = 16 \times 32 + 32$$

$$parameters_3 = 32 \times 1 + 1$$


> From above
We can see that after ten trainings, the validation loss value is very close to the train. Let's see the visualization.

```python
plt.plot(history.history["loss"], label = "training loss value")
plt.plot(history.history["val_loss"], label = "validation loss value")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.ylim(0,4)
plt.legend()
plt.show()
```

![tf3.jpg]({{ site.baseurl }}/images/tf4.png)


We can the loss value is very close to each other, which means very satisfied. Let's see what's our relationship between the predict value and the true value.

```python
predict = model.predict(x)
predict = predict.reshape(-1)
plt.scatter(iris["Sepal.Length"], iris["Petal.Length"], label = "True value")
plt.plot(iris["Sepal.Length"], predict, color = "orange", label = "predict")
plt.legend()
plt.show()
```
![tf3.jpg]({{ site.baseurl }}/images/tf5.png)

From our plot, we can see it's just a straight line, so Nerual Networks treat it as a linear regression in some ways. But this line is little bit different from the linear regression line, the reason is that because in Nerual Networks, we split our data into validation and train, so the result is more suitable for test data.


## D). Conclusion

This is the introduction of TensorFlow and what's Nerual Networks. And Nerual Networks can deal with many problems like regression and classification. Here, I just introduce the regression. Nerual Networks can do many works based on different layers. In next post, I am going to discuss the [**Convolutional Neural Network**](https://en.wikipedia.org/wiki/Convolutional_neural_network), the biggest difference between CNN and today's Neural Network is the layers. A CNN typically has three layers: a convolutional layer, a pooling layer, and a fully connected layer.


Anyway, I am going to discuss Convolutional Neural Network in Deep Learning.

