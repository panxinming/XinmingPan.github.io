---
layout: post
title:  TensorFlow 2.0
---


[TensorFlow](https://www.tensorflow.org/) is a free and open-source software library for machine learning and artificial intelligence. It can be used across a range of tasks but has a particular focus on training and inference of deep neural networks. 

Here, I want to introduce what is **TensorFlow** and What is **Neural Networks** in details to help you understand clearly. And I will use the TensorFlow package for advanced machine learning, with an emphasis on neural networks.


## A). TensorFlow

Before we get in touch with TensorFlow, we must know what is Tensor.


### I. Tensors

> So, uh, what's a tensor? 

In simple words, A tensor is a container which can hold data in $$N$$ dimensions. Tensors are generalizations of matrices to $$N$$-dimensional space. For example, when $$N=1$$, a tensor is a scalar. When $$N=2$$, a tensor is a vector.  

![tf1.jpg]({{ site.baseurl }}/images/tf1.png)


<br>

In Python, A tensor is pretty much just a **Numpy** array. 

Most of the time, we'll work with 2-dimensional tensors (matrices), with the occasional 3-dimensional tensor thrown in for spice.



### II. Structure of Neural Networks

Before we start working with **TensorFlow**, I'd like to build up our understanding of what a neural network actually does to data. We'll see that neural networks are built up of very simple mathematical transformations, which are then stacked on top of each other to produce complex models. 

Suppose I have a data point $$x$$, with 5 features (columns): 

$$X_i = \begin{bmatrix}
x_1 & x_2 & x_3 & x_4 & x_5
\end{bmatrix}$$

A single *unit* of a neural network has two stages: 

1. The unit takes the entries of $$x$$, multiplies them by some *weights*, and adds them together, forming the linear combination $$y = \sum_i w_i x_i$$. This number $$y$$ is often called the *activation*. The number $$w_i$$ is also known as weight.
2. Second, the unit hits $$y$$ with a non-linear function, producing a new output $$z$$: $$z = g(y)$$.


A common nonlinear function is the sigmoid (same function as used in logistic regression): The higher score $$z$$ we have, that means our model is good.



By given a weight vector $$w$$, we can easily compute the layer output  for $$x$$, remembering that `w @ x` will calculate the inner product for us. 

In practice, we perform this computation using many data points and many units simultaneously. In this case, we can represent the data points as an $$m \times n$$ matrix $$\mathbf{X}$$, where $$m$$ is the number of data points and $$n$$ is the number of columns. We can represent the matrix of weights as an $$p \times n$$ matrix $$\mathbf{W}$$, where $$p$$ is the number of units in the layer. We can then compute all the activations simultaneously by computing the matrix product $$\mathbf{Y} = \mathbf{W} \mathbf{X}^T$$. 

![](https://www.jeremyjordan.me/content/images/2018/01/Screen-Shot-2017-11-07-at-12.53.07-PM.png)

### III. Example in Neural Networks

Suppose we are going to do the image classification by using Neural Networks. We can treat every image as a tensor (As we mentioned above). One image has 3-dimentional tensor. So, it has 
$$ inputSize \times inputSize \times 3$$ entires inside. 

For example, we have one image which has $$32 \times 32 \times 3$$ size. So, it has $$3072$$ entries in total. If this image represents dog. So, we can treat our one observation as 

$$X_i = \begin{bmatrix}
x_1 & x_2 & x_3 & ... & x_{3072}
\end{bmatrix}$$

 And our weight vector as 

 $$W_i = \begin{bmatrix}
w_1 & w_2 & w_3 & ... & w_{3072}
\end{bmatrix}$$

Then,

$$\mathbf{W} \mathbf{X}^T + \mathbf{b} = a$$

The $$a$$ is called activation. After we got our activation, we need to compare our activation by different weighted vector. Suppose we have weighted vector $$W_1 = \begin{bmatrix}
w_{1,1} & w_{1,2} & w_{1,3} & ... & w_{1,3072}
\end{bmatrix}$$ represent dog, $$W_2 = \begin{bmatrix}
w_{2,1} & w_{2,2} & w_{2,3} & ... & w_{2,3072}
\end{bmatrix}$$ represent dog and $$W_3 = \begin{bmatrix}
w_{3,1} & w_{3,2} & w_{3,3} & ... & w_{3,3072}
\end{bmatrix}$$ represent car. So, we have have 

$$\begin{bmatrix}
w_{1,1} & w_{1,2} & w_{1,3} & ... & w_{1,3072} \\
w_{2,1} & w_{2,2} & w_{2,3} & ... & w_{2,3072} \\
w_{3,1} & w_{3,2} & w_{3,3} & ... & w_{3,3072}
\end{bmatrix} \begin{bmatrix}
x_1 \\ x_2 \\ x_3 \\ ... \\ x_{3072}
\end{bmatrix} + \begin{bmatrix}
b_1 \\ b_2 \\ b_3
\end{bmatrix}= \begin{bmatrix}
a_1 \\ a_2 \\ a_3
\end{bmatrix}$$

If $$a_1$$ has the highest activation, so can treat our image as dog. However, as we mentioned above, we need to hits the activation into non-linear function to get a new output $$z$$: $$z = g(y)$$. Most of the Activation Functions can be found in this [link](https://en.wikipedia.org/wiki/Activation_function).

And we also have a function called [loss function](https://en.wikipedia.org/wiki/Loss_function) which can help us determine which image it comes from.

## b). TensorFlow Basics

We can install TensorFlow in our command prompt under the environment Anaconda. Type:
```
pip install tensorflow
```

After we installed, let's check the version.

```python
import tensorflow as tf
tf.__version__
```
```
'2.8.0'
```

<br>

We can create a simple, "constant" tensor using `tf.constant`: 

```python
x = tf.constant([[1,9], [3,6]])
x
```
```
<tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[1, 9],
       [3, 6]])>
```

And we can check it's shape:

```python
x.shape
```
```
TensorShape([2, 2])
```

As you can see from the output, this object has a `shape`, a `dtype`, and even an internal `numpy` representation -- it really is a lot like a `numpy` array! Like `numpy` arrays, we can do various mathematical operations: 