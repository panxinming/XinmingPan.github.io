---
layout: post
title: Linear Regression
---


In statistics, **linear regression** is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables). The case of one explanatory variable is called simple linear regression; for more than one, the process is called multiple linear regression.

In this post, I will show how to use Python to draw the linear regression line and show how it works. Here, I only discuss about the simple linear regression.

$$y = \beta_{0} + \beta_1 x + \varepsilon, \varepsilon \sim N(0, \sigma^2)$$

## 1. Load Data

Here we use the data called **"iris"**.
```python
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sb

# import some data to play with
iris = pd.read_csv("iris.csv")
iris = iris.iloc[:,1:6]
iris
```
{% include iris.html %}

## 2. Draw the Scatter Plot

Here I want to find the relationship between Sepal.Length and Petal.Length, so I create two variables and draw the scatterplt of them.

```python
# Create Variables X and Y
x = iris["Sepal.Length"]
y = iris["Petal.Length"]
# Create the scatter plot
plt.scatter(x,y)
plt.xlabel("Sepal.Length")
plt.ylabel("Petal.Length")
plt.show()
```
![lm7.png]({{ site.baseurl }}/images/lm7.png)


## 3. Create Linear Model

Then we would like to create a linear model to fit our data.

```python
from sklearn.linear_model import LinearRegression
import numpy as np
# You should call .reshape() on x because this array is required to be two-dimensional, 
# or to be more precise, to have one column and as many rows as necessary. 
# That’s exactly what the argument (-1, 1) of .reshape() specifies.
x = x.values.reshape((-1, 1))
y = np.array(y)
```

After we did some transformation in $$x$$ and $$y$$, we are going to create the linear model.
```python
# create linear model
model = LinearRegression()
model.fit(x, y)
# R squared
r_sq = model.score(x, y)
r_sq
```
```
0.759954645772515
```

We first to created ***R squared*** to see if this linear model is good enough. If ***R squared*** is close to 1, that means it's very good, otherwise these two variables are not highly correlated. Here we have 0.76 as our R squared, it's not bad.


## 4. Get Our Predicted Parameters

Now, after we created our linear model. We are going to calculate the Predicted Parameters which are $$\beta_0$$ and $$\beta_1$$. 

Here, I want to introduce three ways to get our estimated parameters.

### a). Python

Python can automatically return us the estimated parameters by using some simple code.

```python
beta_0 = model.intercept_
beta_1 = model.coef_[0]
print('The intercept is :', beta_0)
print('The slope is :', beta_1)
```
```
The intercept is : -7.101443369602453
The slope is : 1.858432978254841
```

After we did the prediction, we get our predicted linear equation which is:

$$\hat{y} = -7.10 x + 1.86$$

### b). Pearson’s Correlation Coefficient

Here is the mathematical formula called "pearson’s correlation coefficient" to compute the estimated parameters $$\beta_0$$ and $$\beta_1$$.


$$\hat{\beta_0} = \frac{\sum{y} \sum{x^2} - \sum{x} \sum{xy}}{n\sum{x^2}-(\sum{x})^2}$$

$$\hat{\beta_1} = \frac{n\sum{xy} - \sum{x}\sum{y}}{n\sum{x^2}-(\sum{x})^2}$$

Let's create some new columns that can help us calculate the estimated parameters easier. See how it looks like.
```python
# create one extra column which is x times y.
iris["xy"] = iris["Sepal.Length"]*iris["Petal.Length"]
# create one extra column which is the square of x.
iris["$x^2$"] = iris["Sepal.Length"]**2
# create one extra column which is the square of y.
iris["$y^2$"] = iris["Petal.Length"]**2
iris
```
{% include iris1.html %}

<br />

Then, we can get our $$\beta_0$$ and $$\beta_1$$ by using the formula.
```python
# calculate beta_0 and beta_1 by using the formula above.
beta_0 = (sum(iris["Petal.Length"])*sum(iris["$x^2$"])-sum(iris["Sepal.Length"])*sum(iris["xy"]))/(150*sum(iris["$x^2$"]) - sum(iris["Sepal.Length"])**2)
beta_1 = (150*sum(iris["xy"]) - sum(iris["Sepal.Length"])*sum(iris["Petal.Length"]))/(150*sum(iris["$x^2$"]) - sum(iris["Sepal.Length"])**2)
print('The intercept is :', beta_0)
print('The slope is :', beta_1)
```
```
The intercept is : -7.101443369602453
The slope is : 1.858432978254841
```

The result is very same as above. So, we can conclude our linear equation is:

$$\hat{y} = -7.10 x + 1.86$$

### d). Least Square

The method we want to use is [**least square method**](https://en.wikipedia.org/wiki/Least_squares). It's a very useful method to compute the estimated parameters.

The formula is below:

$$\hat{\beta_1} = \frac{\sum_{i=1}^n{(x_i-\bar{x})(y_i-\bar{y})}}{\sum_{i=1}^n (x_i-\bar{x})^2}$$

$$\hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x}$$


Now, let's get these two estimated parameters by using Python.
```python
# calculate beta_0 and beta_1 by using the formula above.
beta_1 = sum((iris["Sepal.Length"]-iris["Sepal.Length"].mean())*(iris["Petal.Length"] - iris["Petal.Length"].mean()))/sum((iris["Sepal.Length"]-iris["Sepal.Length"].mean())**2)
beta_0 = iris["Petal.Length"].mean() - beta_1*iris["Sepal.Length"].mean()
print('The intercept is :', beta_0)
print('The slope is :', beta_1)
```
```
The intercept is : -7.10144336960245
The slope is : 1.8584329782548403
```

The result is also very same as above. So, we can conclude our linear equation is:

$$\hat{y} = -7.10 x + 1.86$$

### c). Gradient Descent

Please notice that there is one algorithm called [**gradient descent**](https://en.wikipedia.org/wiki/Gradient_descent) which can help us get the predicted parameters very efficiently. I will discuss this algorithm in later post.

Overall, the linear equation is:

$$\hat{y} = -7.10 x + 1.86$$


> Let's draw the line to see if it fits our data very well.


## 5. Draw the Linear Equation Line

```python
# Create Variables X and Y
x = iris["Sepal.Length"]
y = iris["Petal.Length"]
# Create the scatter plot
plt.scatter(x,y)
# draw the linear equation line
plt.plot(x, beta_1*x+beta_0)
plt.xlabel("Sepal.Length")
plt.ylabel("Petal.Length")
plt.show()
```
![lm8.png]({{ site.baseurl }}/images/lm8.png)

Here we can see the line fit our data very well.


## 6. Three Assumptations

Before we did linear model, we must satisfy three assumptions. The reason of that is because if our data didn't satisfy these three assmuptations, linear model will be the bad choice.

These Three Assumptations are:

1. **Linearity**: The relationship between X and the mean of Y is linear.
2. **Normality**: The residual should follow normal distribution.
3. **Homoscedasticity**: The variance of residual should be constant.

### a). Linearity

We can check this by comparing the fitted value V.S residual, if all the point is approximately symmetric by $$y = 0$$, that means linearity assumption are satisfied. The reason why we choose fitted values instead of X is because when we meet mutivariate linear regression, there are more than one X values. So, fitted value can be a substitute of value X.


So, we are going to draw the residual plot step by step.
```python
# create one column which is precited values
iris["predict"] = (beta_1*x+beta_0)
# compute the residual
iris["residual"] = (y - (beta_1*x+beta_0))
# plot the fitted value V.S residual
plt.scatter(iris["predict"],iris["residual"])
# draw the straight line y = 0
plt.plot([0,8], [0,0], color = "red")
plt.xlabel("fitted value")
plt.ylabel("residual")
plt.title("Residual Plot")
plt.show()
```

![lm9.png]({{ site.baseurl }}/images/lm9.png)


### b). Normality

If linear regression exists, so the residual should follow normal distribution. So, QQ plot is a good way to check if the residual follows normal distribution or not. Here we are going to draw QQ plot step by step.

```python
from scipy.stats import norm
# create a numpy array. The meaning of that is I would like to create 150 perfect Z-scores which from negative infinite to infinite.
# Like the cdf = 0.5, then Z-score = 0; when cdf = 0.975, then Z-score = 1.96.
a = np.linspace(0, 1, 150)
a = norm.ppf(a)
# standardize the residual. In other words, calculate the Z-score of every residual.
b = (iris["residual"] - iris["residual"].mean())/iris["residual"].std()
# sort them
b = np.sort(b)
# draw the QQ normal plot
plt.scatter(a,b)
# draw the QQ normal line
plt.plot([-2.5,2.5], [-2.5,2.5], color = "red")
plt.xlabel("theoretical quantiles")
plt.ylabel("standardized residual")
plt.title("QQ Normal Plot")
plt.show()
```
![lm10.png]({{ site.baseurl }}/images/lm10.png)


### c). Homoscedasticity

In this assumptation, we must satisfy the constant variance assumptation. Here, Scale-Location plot will be a good figue to test if the variance is constant or not. The X-axis is the fitted value (I explained that in part a), and the Y-axis is the square root of the absolute value of the standardized residual.

I am going to draw this plot step by step, so that we can see the details clearly.

```python
# calculate the the square root of the absolute value of the standardized residual
ab_std = abs((iris["residual"] - iris["residual"].mean())/iris["residual"].std())
ab_std = np.sqrt(ab_std)
# plot the Scale-Location plot
plt.scatter(iris["predict"],ab_std)
plt.xlabel("fitted values")
plt.ylabel("$\sqrt{| standardized  residual |}$")
plt.title("Scale-Location Plot")

# draw the trend line.
z = np.polyfit(iris["predict"], ab_std, 3)
p = np.poly1d(z)
plt.plot(iris["predict"],p(iris["predict"]),"r--")

plt.show()
```
![lm11.png]({{ site.baseurl }}/images/lm11.png)

If the trend is almost flat, that means it satisfy the constant variance assumption of linear regression.



## 7. Conclusion

From all above, we can see the constant variance assumptation is not satisfied. So, I think linear regression will not be the best model to predict or describe the data we are using. From my perspective, I think some other algorithms may be better to desribe the data, like [Cluster analysis](https://en.wikipedia.org/wiki/Cluster_analysis). Because there are three species in our data. And [polynomial regression](https://en.wikipedia.org/wiki/Polynomial_regression) may be the good model to predict and fit our data. Finally, We can still insist on using linear regression, that's possible, but the result may not be as good as we predict.