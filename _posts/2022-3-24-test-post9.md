---
layout: post
title: Linear Regression
---


In statistics, **linear regression** is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables). The case of one explanatory variable is called simple linear regression; for more than one, the process is called multiple linear regression.

In this post, I will show how to use Python to draw the linear regression line and show how it works. Here, I only discuss about the simple linear regression.

$$y = \beta_{0} + \beta_1 x + \varepsilon, \varepsilon \sim N(0, \sigma^2)$$

## 1. Load Data

Here we use the data called **"iris"**.
```python
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sb

# import some data to play with
iris = pd.read_csv("iris.csv")
iris = iris.iloc[:,1:6]
iris
```
{% include iris.html %}

## 2. Draw the Scatter Plot

Here I want to find the relationship between Sepal.Length and Petal.Length, so I create two variables and draw the scatterplt of them.

```python
# Create Variables X and Y
x = iris["Sepal.Length"]
y = iris["Petal.Length"]
# Create the scatter plot
plt.scatter(x,y)
plt.xlabel("Sepal.Length")
plt.ylabel("Petal.Length")
plt.show()
```
![lm7.png]({{ site.baseurl }}/images/lm7.png)


## 3. Create Linear Model

Then we would like to create a linear model to fit our data.

```python
from sklearn.linear_model import LinearRegression
import numpy as np
# You should call .reshape() on x because this array is required to be two-dimensional, 
# or to be more precise, to have one column and as many rows as necessary. 
# Thatâ€™s exactly what the argument (-1, 1) of .reshape() specifies.
x = x.values.reshape((-1, 1))
y = np.array(y)
```

After we did some transformation in $$x$$ and $$y$$, we are going to create the linear model.
```python
# create linear model
model = LinearRegression()
model.fit(x, y)
# R squared
r_sq = model.score(x, y)
r_sq
```
```
0.759954645772515
```

We first to created ***R squared*** to see if this linear model is good enough. If ***R squared*** is close to 1, that means it's very good, otherwise these two variables are not highly correlated. Here we have 0.76 as our R squared, it's not bad.


## 4. Get Our Predicted Parameters

Now, after we created our linear model. We are going to calculate the Predicted Parameters which are $$\beta_0$$ and $$\beta_1$$. Please notice that there is one algorithm called [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) which can help us get the predicted parameters very efficiently. I will discuss this algorithm in later post.

```python
beta_0 = model.intercept_
beta_1 = model.coef_[0]
print('The intercept is :', beta_0)
print('The slope is :', beta_1)
```
```
The intercept is : -7.101443369602453
The slope is : 1.858432978254841
```

After we did the prediction, we get our predicted linear equation which is:

$$\hat{y} = -7.10 x + 1.86$$

Let's draw the line to see if it fits our data very well.


## 5. Draw the Linear Equation Line

```python
# Create Variables X and Y
x = iris["Sepal.Length"]
y = iris["Petal.Length"]
# Create the scatter plot
plt.scatter(x,y)
# draw the linear equation line
plt.plot(x, beta_1*x+beta_0)
plt.xlabel("Sepal.Length")
plt.ylabel("Petal.Length")
plt.show()
```
![lm8.png]({{ site.baseurl }}/images/lm8.png)

Here we can see the line fit our data very well.


## 6. Three Assumptations

Before we did linear model, we must satisfy three assumptions. The reason of that is because if our data didn't satisfy these three assmuptations, linear model will be the bad choice.

These Three Assumptations are:

1. **Linearity**: The relationship between X and the mean of Y is linear.
2. **Normality**: For any fixed value of X, Y is normally distributed.
3. **Homoscedasticity**: The variance of residual is the same for any value of X.