---
layout: post
title:  K-Nearest Neighbors (KNN)
---


In statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method. It is used for classification and regression. In both cases, the input consists of the k closest training examples in a data set. The output depends on whether k-NN is used for classification or regression.


In this post, I will show you how to write KNN Algorithm step by step in Python and understand how KNN works.


> **KNN**

## 1. Basic Idea

Here, I will teach the KNN for classfication. Suppose we have a data like this image below. The red, yellow and green points are three different clusters. The Mark "x" is the point that we want to predict.

![knn1.png]({{ site.baseurl }}/images/knn1.png)


Here, we can see the image has five lines across the black mark. Remember KNN algorithm has an important parameters called $$k$$. Here, because it has five lines, so the $$k = 5$$. That means among these five lines, three of them are pointed to the red cluster. So, the black mark is belonged to the red cluster. That how KNN works.


## 2. KNN Algorithm

Here, I want to show how to write KNN in Python in details. And we are going to use the data "Iris" again.

### a). Load Data

```python
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# import some data to play with
iris = pd.read_csv("iris.csv")
iris = iris.iloc[:,1:6]
# Only consider the Species "versicolor" and "virginica".
# So, we only consider two clusters.
iris = iris[iris["Species"] != "versicolor"]
iris["Species"] = iris["Species"].replace(["setosa", "virginica"], [0,1])
iris
```
{% include iris3.html %}

### b). Data Visualization
After we load the data and do the data cleaning. Let's do the Data Visualization to see how these two clusters are distributed by comparing Sepal.Length to Sepal.Width.

```python
x1 = iris[iris["Species"] == 0]["Sepal.Length"]
y1 = iris[iris["Species"] == 0]["Sepal.Width"]
x2 = iris[iris["Species"] == 1]["Sepal.Length"]
y2 = iris[iris["Species"] == 1]["Sepal.Width"]
plt.scatter(x1,y1, c="red", label = "setosa")
plt.scatter(x2,y2, c="yellow", label = "virginica")
plt.xlabel("Sepal.Length")
plt.ylabel("Sepal.Width")
plt.legend()
plt.show()
```
![knn2.png]({{ site.baseurl }}/images/knn2.png)


### b). KNN Function

In this part, I am going to write my personal KNN function in Python. I will use the data "Iris" as my train data.

```python
def my_knn(x,y,k):
    length = {}
    a = np.array(iris["Sepal.Length"])
    b = np.array(iris["Sepal.Width"])
    for i in range(len(iris)):
        c = np.sqrt((x-a[i])**2 + (y-b[i])**2)
        length[i] = c
    length = dict(sorted(length.items(), key=lambda item: item[1]))
    
    location = list(length.keys())[0:k]
    
    cluster_red = 0
    cluster_yellow = 0
    for j in location:
        if list(iris[j:(j+1)]["Species"])[0] == 0:
            cluster_red += 1
        else:
            cluster_yellow += 1
            
    if cluster_red > cluster_yellow:
        print("The point is belong to the cluster red, because in the cloest", k, "points," , cluster_red, "of them are red")
    else:
        print("The point is belong to the cluster yellow, because in the cloest", k, "points," , cluster_yellow, "of them are yellow")
```

<br />

After we complete our personal KNN function, let's try some points.

```python
my_knn(5.7,3,10)
```
```
The point is belong to the cluster yellow, because in the cloest 10 points, 8 of them are yellow
```

Let's see how it shows in graph.
```python
x1 = iris[iris["Species"] == 0]["Sepal.Length"]
y1 = iris[iris["Species"] == 0]["Sepal.Width"]
x2 = iris[iris["Species"] == 1]["Sepal.Length"]
y2 = iris[iris["Species"] == 1]["Sepal.Width"]
plt.scatter(x1,y1, c="red", label = "setosa")
plt.scatter(x2,y2, c="yellow", label = "virginica")
plt.scatter(5.7,3, c="green")
plt.xlabel("Sepal.Length")
plt.ylabel("Sepal.Width")
plt.legend()
plt.show()
```
![knn3.png]({{ site.baseurl }}/images/knn3.png)


We can see the green point that we created is very close to the yellow cluster. And our KNN function also give us the correct output. So, that's how KNN works.



### c). Decision Region (Decision Boundary)

There is one thing we must know is that. The decision boundary will become more and more complex when $$k$$ grows up. If our data is very huge, so big $$k$$ may do the classfication very well. If our data is not big, simple $$k$$ may give us better result than big $$k$$.

Here I will to show the decision boundary affect by $$k$$.

I will write my personal function for the decision boundary.

```python
def my_knn1(x,y,k):
    length = {}
    a = np.array(iris["Sepal.Length"])
    b = np.array(iris["Sepal.Width"])
    for i in range(len(iris)):
        c = np.sqrt((x-a[i])**2 + (y-b[i])**2)
        length[i] = c
    length = dict(sorted(length.items(), key=lambda item: item[1]))
    
    location = list(length.keys())[0:k]
    
    cluster_red = 0
    cluster_yellow = 0
    for j in location:
        if list(iris[j:(j+1)]["Species"])[0] == 0:
            cluster_red += 1
        else:
            cluster_yellow += 1     
    return cluster_red, cluster_yellow



def decision_region(k):
    line_x = np.arange(4,8,0.05)
    line_y = np.arange(2,4.5,0.05)
    points_x = []
    points_y = []
    for i in line_x:
        for j in line_y:
            if my_knn1(i,j,k)[0] == my_knn1(i,j,k)[1] or my_knn1(i,j,k)[0] == my_knn1(i,j,k)[1]-1 or my_knn1(i,j,k)[0] == my_knn1(i,j,k)[1]+1:
                points_x.append(i)
                points_y.append(j)
    return points_x, points_y
```

First, we choose $$k = 10$$. The decision boundary seems very well.

```python
x1 = iris[iris["Species"] == 0]["Sepal.Length"]
y1 = iris[iris["Species"] == 0]["Sepal.Width"]
x2 = iris[iris["Species"] == 1]["Sepal.Length"]
y2 = iris[iris["Species"] == 1]["Sepal.Width"]
plt.scatter(x1,y1, c="red", label = "setosa")
plt.scatter(x2,y2, c="yellow", label = "virginica")
plt.plot(np.sort(decision_region(10)[0]),np.sort(decision_region(10)[1]))
plt.xlabel("Sepal.Length")
plt.ylabel("Sepal.Width")
plt.legend()
plt.show()
```
![knn4.png]({{ site.baseurl }}/images/knn4.png)


Then, we choose $$k = 70$$. The decision boundary appears lots of mistakes. Like there are some yellow points considered as red cluster. So, according to small sample. if $$k$$ is big, the decision boundary will become complex and it will be not accuracy. So, we must choose the appropriate $$k$$ for our data.

```python
x1 = iris[iris["Species"] == 0]["Sepal.Length"]
y1 = iris[iris["Species"] == 0]["Sepal.Width"]
x2 = iris[iris["Species"] == 1]["Sepal.Length"]
y2 = iris[iris["Species"] == 1]["Sepal.Width"]
plt.scatter(x1,y1, c="red", label = "setosa")
plt.scatter(x2,y2, c="yellow", label = "virginica")
plt.plot(np.sort(decision_region(70)[0]),np.sort(decision_region(70)[1]))
plt.xlabel("Sepal.Length")
plt.ylabel("Sepal.Width")
plt.legend()
plt.show()
```
![knn5.png]({{ site.baseurl }}/images/knn5.png)



## 3. Conclusion

k-NN is a type of classification where the function is only approximated locally and all computation is deferred until function evaluation. Since this algorithm relies on distance for classification, if the features represent different physical units or come in vastly different scales then normalizing the training data can improve its accuracy dramatically.

Compared to other classification or regression algorithm, KNN is not associated with much mathematical knowledge. Anyway, KNN is a very important algorithm in Machine Learning.




