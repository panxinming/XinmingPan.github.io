---
layout: post
title:  K-Means Clustering
---

k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster.

K-means is unsupervised machine learning method.

I will introduce how K-means works in Python specifically.


## 1. Create Clusters

K-means is a cluster algorithm. So, here I will use a package in Python that can help us randomly generate clusters. Here, I create 1000 points and classfy them into 2 clusters.
```python
from sklearn.datasets import make_blobs
x, y = make_blobs(n_samples = 1000, n_features = 2, centers = 2,random_state=426)
plt.scatter(x[:,0],x[:,1])
```


![kmean1.png]({{ site.baseurl }}/images/kmean1.png)



## 2. Formula and Pseudocode

The K-means algorithm  aims at  minimizing  an objective function know as squared error function given by: 

$$J(V) = \sum_{i=1}^{n} \sum_{j=1}^{m} (\| x_i - V_j \|)^2$$

Where:

$$\| x_i - V_j \|^2$$ is the Euclidean distance between $$x_i$$ and $$V_j$$.

$$V_j$$ is the central point of $$j th$$ cluster.


### Algorithmic steps for k-means clustering

- Randomly select cluster centers.
- Calculate the distance between each data point and cluster centers.
- Assign the data point to the cluster center whose distance from the cluster center is minimum of all the cluster centers..
- Recalculate the new cluster center using:

$$V_i = \frac{1}{m} \sum_{j=1}^{m} x_i$$

- Recalculate the distance between each data point and new obtained cluster centers.
- If no data point was reassigned then stop, otherwise repeat from step 3).


After understandting the process, let's do the clustering in Python step by step.


## 3. Randomly select cluster centers

The main idea of K-mean is to randomly choose 2 clusters as our own clusters. Then, calcuate the distance between every point and the central of every cluster. So, here we randomly create two clusters by our own.

From the graph, we can see that our cluster is bad. Because we can not distinguish these two clusters.
```python
cluster_1 = x[0:500,:]
cluster_2 = x[500:1000,:]
plt.scatter(cluster_1[:,0], cluster_1[:,1], color = "orange")
plt.scatter(cluster_2[:,0], cluster_2[:,1], color = "green")
plt.show()
```
![kmean2.png]({{ site.baseurl }}/images/kmean2.png)

From this graph, we can see that these two clusters will have different central points.


## 4. Central Points

Because we want to find the central points of clusters, so here we prefer to create a function to get the central point of each cluster.

```python
def central_point(x):
    a = np.sum(x[:,0])/len(x[:,0])
    b = np.sum(x[:,1])/len(x[:,1])
    return [a,b]
```


## 5. Clustering
After we have these 2 new centroids, a new binding has to be done  between  the same data set points  and  the nearest new center.  A loop has been generated.  As a result of  this loop we  may  notice that the 2 centers change their location step by step until no more changes  are done or  in  other words centers do not move any more.

```python
i = 0
# create an empty list
hhh = []
while True:
    i += 1
    # calculate the distance of every point to the clsuter A
    distance_a = (cluster_1[i,0] - central_point(cluster_1)[0])**2 + (cluster_1[i,1] - central_point(cluster_1)[1])**2
    distance_a = np.sqrt(distance_a)
    # calculate the distance of every point to the clsuter B
    distance_b = (cluster_1[i,0] - central_point(cluster_2)[0])**2 + (cluster_1[i,1] - central_point(cluster_2)[1])**2
    distance_b = np.sqrt(distance_b)
    # If the distance between the point and Cluster A is bigger # than the distance between the point and Cluster B. Then, # assign this point to the cluster B. Otherwise, stay in #Cluster A.
    if distance_a >= distance_b:
        cluster_2 = np.append(cluster_2,[cluster_1[i,:]],axis=0)
        # record which points in Cluster A are assigned to Cluster B
        # We need to remove this points from Cluster A.  
        hhh.append(i)
    if i == len(cluster_1)-1:
        break
# remove these points
cluster_1 = np.delete(cluster_1, hhh,0)

# let's see how it looks like.
plt.scatter(cluster_1[:,0], cluster_1[:,1], color = "orange")
plt.scatter(cluster_2[:,0], cluster_2[:,1], color = "green")
plt.show()
```

![kmean3.png]({{ site.baseurl }}/images/kmean3.png)



It looks better. Now let's modify Cluster B.

```python
ggg = []
i = 0
while True:
    i += 1
    distance_a = (cluster_2[i,0] - central_point(cluster_1)[0])**2 + (cluster_2[i,1] - central_point(cluster_1)[1])**2
    distance_a = np.sqrt(distance_a)
    distance_b = (cluster_2[i,0] - central_point(cluster_2)[0])**2 + (cluster_2[i,1] - central_point(cluster_2)[1])**2
    distance_b = np.sqrt(distance_b)
    if distance_a <= distance_b:
        cluster_1 = np.append(cluster_1,[cluster_2[i,:]],axis=0)
        ggg.append(i)
    if i == len(cluster_2)-1:
        break
cluster_2 = np.delete(cluster_2, ggg,0) 

plt.scatter(cluster_1[:,0], cluster_1[:,1], color = "orange")
plt.scatter(cluster_2[:,0], cluster_2[:,1], color = "green")
plt.show()
```

![kmean4.png]({{ site.baseurl }}/images/kmean4.png)

Now, our data is successfully get clustered. This is how K-means Algorithm Works in details. In Python, we already have K-mean function from Sklearn that can help us clustering more efficent. Let's see how it works.

```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2)
km.fit(x)

plt.scatter(x[:,0], x[:,1], c = km.predict(x))
```

![kmean5.png]({{ site.baseurl }}/images/kmean5.png)



## 6. Conclusion
Before I conclude K-means algorithm, I want to introduce some disadvantages of K-mean clustering algorithm.

- The learning algorithm requires specification of the number of  cluster centers.
- If there are two highly overlapping data then k-means will not be able to resolve
- Unable to handle noisy data and outliers.
- Unable to handle "wired" data.


Overall, K-mean clustering algorithm has a lot of benefits and it can help us deal with many probblems. However, it has some limits, so in the next post, I am going to introduce one advanced clustering algoritm which is the optimization of K-mean. It's called [Spectral clustering](https://en.wikipedia.org/wiki/Spectral_clustering).