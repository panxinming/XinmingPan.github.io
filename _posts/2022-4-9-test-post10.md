---
layout: post
title: Decision Tree
---


A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.

Here, I will introduce how Decision Tree works in detail and draw Decision Tree step by step in Python. Decision Tree is a very important model in Machine Learning Model.

First, I want to show one picture to give an idea that how Decision Tree works.

![dt1.png]({{ site.baseurl }}/images/dt1.png)

## 1. Load Data
 We will still use the data "Iris". We want to use four features to predict the speices of Iris.

 ```python
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# import some data to play with
iris = pd.read_csv("iris.csv")
iris = iris.iloc[:,1:6]
iris
 ```

{% include iris.html %}


```python
# Only consider the Species "setosa" and "virginica"
iris = iris[iris["Species"] != "versicolor"]
iris["Species"] = iris["Species"].replace(["setosa", "virginica"], [0,1])

# change these four features into binary variables.
iris["Sepal.Length"] = iris["Sepal.Length"] <= np.mean(iris["Sepal.Length"])
iris["Sepal.Width"] = iris["Sepal.Width"] <= np.mean(iris["Sepal.Width"])
iris["Petal.Length"] = iris["Petal.Length"] <= np.quantile(iris["Petal.Length"], 0.25)
iris["Petal.Width"] = iris["Petal.Width"] <= np.quantile(iris["Petal.Width"], 0.55)
iris
```

{% include iris2.html %}

<br />

## 2. Entropy
Entropy is an information theory metric that measures the impurity or uncertainty in a group of observations. It determines how a decision tree chooses to split data. The image below gives a better description of the purity of a set. If Entropy is close to 1, that means it's very impure. Otherwise, it's not impure.

![dt2.png]({{ site.baseurl }}/images/dt2.png)

The formula of **Entropy** is:

$$H(X) = -\sum_{i=1}^{n} p(x_i) log_{2}(p(x_i))$$

In this formula, $$X$$ means random variable. In classfication problems, we treat $$n$$ as how many groups in this random variable.


In our data, the Species is our target variable, so we can treat Species as random variable $$Y$$, it can be 0 or 1. So, Let's calculate $$p(y=1)$$ and $$p(y=0)$$.

```python
p_1 = sum(iris["Species"] == 1)/len(iris["Species"])
p_0 = 1-p_1

print("The probability when y = 1 is ", p_1)
print("The probability when y = 0 is ", p_0)
```
```
The probability when y = 1 is  0.5
The probability when y = 0 is  0.5
```

So, here our Entropy is:

$$H(Y)= -\sum_{i=1}^{2} p(y_i) log_{2}(p(y_i))$$

$$\hspace{5.4cm} =-(p(y_1) log_{2}(p(y_1)) + p(y_0) log_{2}(p(y_0)))$$

$$\hspace{2.6cm}=-(\frac{1}{2}log_2(\frac{1}{2}) + \frac{1}{2}log_2(\frac{1}{2}))$$

$$\hspace{-0.7cm}=-(-\frac{1}{2} -\frac{1}{2})$$

$$\hspace{-2.13cm}=1$$


So, from the Entropy, we can see our data is very impure. Because the Entropy is 1.


## 3. Information Gain
We can define information gain as a measure of how much information a feature provides about a class. Information gain helps to determine the order of attributes in the nodes of a decision tree.

The main node is referred to as the parent node, whereas sub-nodes are known as child nodes. We can use information gain to determine how good the splitting of nodes in a decision tree.
It can help us determine the quality of splitting, as we shall soon see. The calculation of information gain should help us understand this concept better.

Here I will show three figures to show what are main node (root node), parent node and children node meaning.

![dt3.png]({{ site.baseurl }}/images/dt3.png)
![dt4.png]({{ site.baseurl }}/images/dt4.png)
![dt5.png]({{ site.baseurl }}/images/dt5.png)


So, the formula of **Information Gain** is:

$$Gain = E_{parent} - E_{children}$$


$$E_{children} = \sum_{i=0}^{n}H(y_j)p(x_i), j \in (0,1)$$


Now, let's write our Entropy Function and Information Gain Function in Python.

```python
def get_entropy(df):
    '''
    First, this is the function that can calculate the Entropy 
    of y. The input must be a dataframe, and the last column 
    must be y.
    '''
    y = df[df.columns[len(df.columns)-1]]
    set_y = list(set(y))
    prob_y1 = sum(y == set_y[0])/len(y)
    prob_y0 = 1-prob_y1
    
    # calculate entropy by using formula above.
    if prob_y1 == 0 or prob_y0 == 0:
        entropy = 0
    else:
        entropy = -(prob_y1*np.log2(prob_y1) + prob_y0*np.log2(prob_y0))
    return entropy



def gain_information(df, col):
    '''
    This function is about to calculate the information gain of
    each column. The more information gain we get, we will
    choose this column as our first node.
    '''
    col_entropy = 0
    for value in set(df[col]):
        x_value = df[df[col] == value]
        prob = len(x_value)/len(df)
        entropy = get_entropy(x_value)
        col_entropy += prob*entropy
        
    gain = get_entropy(df) - col_entropy
    return gain
```


Then, calculate the Information Gain of each columns to see which columns give us the highest Information Gain value.

```python
for names in iris.columns:
    print("The Information Gain of ", names, 
          " is ", gain_information(iris, names))
```
```
The Information Gain of  Sepal.Length  is  0.7644028051470533
The Information Gain of  Sepal.Width  is  0.19693633895102614
The Information Gain of  Petal.Length  is  0.537298906440757
The Information Gain of  Petal.Width  is  0.7582766571931676
The Information Gain of  Species  is  1.0
```

We can ignore the last column beacuse this is the variable that we want to classify. So, we only need to compare the first four features.

<br />

## 4. Node and Leaf

Then, we are going to write a class which can help us create Node and Leaf. It's very simple.

```python
class Node():
    def __init__(self, col):
        self.col = col
        self.children = {}
    
    def __str__(self):
        return 'Node col = ' +str(self.col)
    
    
class leaf():
    def __init__(self,y):
        self.y = y
        
    def __str__(self):
        return "Leaf y = " +str(self.y)
    
root = Node("Sepal.Length")
print(root)
```
```
Node col = Sepal.Length
```

<br />


## 5. Decision Tree

Write a function called print_tree, this function can help us print a decision tree. So that we can visulize.

```python
def print_tree(node, prefix = "", subfix = ""):
    prefix += "-" * 4
    print(prefix, node, subfix)
    if isinstance(node, leaf):
        return
    for i in node.children:
        subfix = "value = " + str(i)
        print_tree(node.children[i], prefix, subfix)
```

Then, write a function that can help us find the maximum Information Gain of specific column and print it.
```python        
def get_split_col(df):
    best_gain = 0
    best_col = 0
    for col in df.columns[0:(df.shape[1]-1)]:
        gain = gain_information(df, col)
        
        if gain > best_gain:
            best_gain = gain
            best_col = col
            
    return best_col
```

After we get node, we are going to create children of the parent node. So, this function below is to create children nodes. If all the $$y$$ value is the same after we split the column. We can directly treat the children as leaf instead of node.
```python
def create_children(df, parent_node):
    for value in np.unique(df[parent_node.col]):
        sub_x = df[df[parent_node.col] == value]
        unique_y = np.unique(sub_x[sub_x.columns[len(sub_x.columns)-1]])
        
        if len(unique_y) == 1:
            parent_node.children[value] =leaf(unique_y[0])
            continue
            
        split_col = get_split_col(sub_x)
        parent_node.children[value] = Node(col = split_col)
```

### I. Root Node
Let's plot our root node (Main Node):

```python
create_children(iris, root)
print_tree(root)
```
```
---- Node col = Sepal.Length 
-------- Node col = Petal.Length value = False
-------- Node col = Petal.Width value = True
```

This result means the decision tree select Sepal.Length as our root node, then it creates two nodes instead of leaf. That means we still need to create more nodes and leaf.


### II. Final Decision Tree
I will show you the final result, after we created many nodes and leaves.
```python
x_0_0 = iris[iris[iris.columns[0]] == 0]  
create_children(x_0_0, root.children[0])

x_0_1 = iris[iris[iris.columns[0]] == 1]
create_children(x_0_1, root.children[1])


x_0_2 = iris[iris[iris.columns[3]] == 1]
create_children(x_0_2, root.children[1].children[1])

x_0_3 = iris[iris[iris.columns[2]] == 1]
create_children(x_0_3, root.children[1].children[1].children[0])

print_tree(root)
```
```
---- Node col = Sepal.Length 
-------- Node col = Petal.Length value = False
------------ Leaf y = 1 value = False
------------ Leaf y = 0 value = True
-------- Node col = Petal.Width value = True
------------ Leaf y = 1 value = False
------------ Node col = Petal.Length value = True
---------------- Node col = Sepal.Length value = False
-------------------- Leaf y = 0 value = False
-------------------- Leaf y = 0 value = True
---------------- Leaf y = 0 value = True
```


This result means if you see a node, then you need to go next. Once you see the leaf, that means this is the end of one branch.
I will show one plot that can help us understand this result more clearly.


![dt6.png]({{ site.baseurl }}/images/dt6.png)


<br />


## 6. Conclusion

Decision Tree is a very important machine learning model to deal with classfication problems. This post is about how to write Decision Tree by using [ID3 Algorithm](https://en.wikipedia.org/wiki/ID3_algorithm). And Decision Tree model can also by written by [C4.5 Algoritm](https://en.wikipedia.org/wiki/C4.5_algorithm). I am not going to write this algorithm here, but the main difference between ID3 Decision Tree and C4.5 Decision Tree is they have different ways to calculate the Information Gain. In later post, I will discuss more complex Machine Learning Model.
