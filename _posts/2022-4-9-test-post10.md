---
layout: post
title: Decision Tree
---


A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.

Here, I will introduce how Decision Tree works in detail and draw Decision Tree step by step in Python. Decision Tree is a very important model in Machine Learning Model.

First, I want to show one picture to give an idea that how Decision Tree works.

![dt1.png]({{ site.baseurl }}/images/dt1.png)

## 1. Load Data
 We will still use the data "Iris". We want to use four features to predict the speices of Iris.

 ```python
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# import some data to play with
iris = pd.read_csv("iris.csv")
iris = iris.iloc[:,1:6]
iris
 ```

{% include iris.html %}


```python
# Only consider the Species "setosa" and "virginica"
iris = iris[iris["Species"] != "versicolor"]
iris["Species"] = iris["Species"].replace(["setosa", "virginica"], [0,1])

# change these four features into binary variables.
iris["Sepal.Length"] = iris["Sepal.Length"] <= np.mean(iris["Sepal.Length"])
iris["Sepal.Width"] = iris["Sepal.Width"] <= np.mean(iris["Sepal.Width"])
iris["Petal.Length"] = iris["Petal.Length"] <= np.quantile(iris["Petal.Length"], 0.25)
iris["Petal.Width"] = iris["Petal.Width"] <= np.quantile(iris["Petal.Width"], 0.55)
iris
```

{% include iris2.html %}

<br />

## 2. Entropy
Entropy is an information theory metric that measures the impurity or uncertainty in a group of observations. It determines how a decision tree chooses to split data. The image below gives a better description of the purity of a set. If Entropy is close to 1, that means it's very impure. Otherwise, it's not impure.

![dt2.png]({{ site.baseurl }}/images/dt2.png)

The formula of **Entropy** is:

$$H(X) = -\sum_{i=1}^{n} p(x_i) log_{2}(p(x_i))$$

In this formula, $$X$$ means random variable. In classfication problems, we treat $$n$$ as how many groups in this random variable.


In our data, the Species is our target variable, so we can treat Species as random variable $$Y$$, it can be 0 or 1. So, Let's calculate $$p(y=1)$$ and $$p(y=0)$$.

```python
p_1 = sum(iris["Species"] == 1)/len(iris["Species"])
p_0 = 1-p_1

print("The probability when y = 1 is ", p_1)
print("The probability when y = 0 is ", p_0)
```
```
The probability when y = 1 is  0.5
The probability when y = 0 is  0.5
```

So, here our Entropy is:

$$H(Y)= -\sum_{i=1}^{2} p(y_i) log_{2}(p(y_i))$$

$$\hspace{5.4cm} =-(p(y_1) log_{2}(p(y_1)) + p(y_0) log_{2}(p(y_0)))$$

$$\hspace{2.6cm}=-(\frac{1}{2}log_2(\frac{1}{2}) + \frac{1}{2}log_2(\frac{1}{2}))$$

$$\hspace{-0.7cm}=-(-\frac{1}{2} -\frac{1}{2})$$

$$\hspace{-2.13cm}=1$$


So, from the Entropy, we can see our data is very impure. Because the Entropy is 1.


## 3. Information Gain
We can define information gain as a measure of how much information a feature provides about a class. Information gain helps to determine the order of attributes in the nodes of a decision tree.

The main node is referred to as the parent node, whereas sub-nodes are known as child nodes. We can use information gain to determine how good the splitting of nodes in a decision tree.
It can help us determine the quality of splitting, as we shall soon see. The calculation of information gain should help us understand this concept better.

Here I will show three figures to show what are main node (root node), parent node and children node meaning.

![dt3.png]({{ site.baseurl }}/images/dt3.png)
![dt4.png]({{ site.baseurl }}/images/dt4.png)
![dt5.png]({{ site.baseurl }}/images/dt5.png)


So, the formula of **Information Gain** is:

$$Gain = E_{parent} - E_{children}$$


$$E_{children} = \sum_{i=0}^{n}H(y_j)p(x_i), j \in (0,1)$$


Now, let's write our Entropy Function and Information Gain Function in Python.

```python
def get_entropy(df):
    '''
    First, this is the function that can calculate the Entropy 
    of y. The input must be a dataframe, and the last column 
    must be y.
    '''
    y = df[df.columns[len(df.columns)-1]]
    set_y = list(set(y))
    prob_y1 = sum(y == set_y[0])/len(y)
    prob_y0 = 1-prob_y1
    
    # calculate entropy by using formula above.
    if prob_y1 == 0 or prob_y0 == 0:
        entropy = 0
    else:
        entropy = -(prob_y1*np.log2(prob_y1) + prob_y0*np.log2(prob_y0))
    return entropy



def gain_information(df, col):
    '''
    This function is about to calculate the information gain of
    each column. The more information gain we get, we will
    choose this column as our first node.
    '''
    col_entropy = 0
    for value in set(df[col]):
        x_value = df[df[col] == value]
        prob = len(x_value)/len(df)
        entropy = get_entropy(x_value)
        col_entropy += prob*entropy
        
    gain = get_entropy(df) - col_entropy
    return gain
```


Then, calculate the Information Gain of each columns to see which columns give us the highest Information Gain value.

```python
for names in iris.columns:
    print("The Information Gain of ", names, 
          " is ", gain_information(iris, names))
```
```
The Information Gain of  Sepal.Length  is  0.7644028051470533
The Information Gain of  Sepal.Width  is  0.19693633895102614
The Information Gain of  Petal.Length  is  0.537298906440757
The Information Gain of  Petal.Width  is  0.7582766571931676
The Information Gain of  Species  is  1.0
```

We can ignore the last column beacuse this is the variable that we want to classify. So, we only need to compare the first four features.