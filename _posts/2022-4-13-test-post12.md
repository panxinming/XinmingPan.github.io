---
layout: post
title:  Random Forest
---


Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the mean or average prediction of the individual trees is returned.


I will show you one picture to see how Random Forest works in details.


![rf1.jpg]({{ site.baseurl }}/images/rf1.jpg)


## 1. Bootstrap Method
Bootstrapping is any test or metric that uses random sampling with replacement, and falls under the broader class of resampling methods. Bootstrapping assigns measures of accuracy (bias, variance, confidence intervals, prediction error, etc.) to sample estimates. This technique allows estimation of the sampling distribution of almost any statistic using random sampling methods.

So, this is the basic method that we used in Random Forest. To get higher accuracy, we use Bootstrap Method to randomly select data to create lots of trees. So, that's called Random Forest.

![rf2.png]({{ site.baseurl }}/images/rf2.png)

## 2. Bagging
Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach.

After we Bootstrap our original data, we will get a new sample. We will use this sample to create trees. After we create enough trees, we will get our final result based on these trees.

![rf3.png]({{ site.baseurl }}/images/rf3.png)

## 3. Random Forest

I will show how random forest works in Python. The "Iris" data will be used to build our model.

### a). Load Data
```python
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# import some data to play with
iris = pd.read_csv("iris.csv")
iris = iris.iloc[:,1:6]
# Only consider the Species "versicolor" and "virginica"
iris = iris[iris["Species"] != "versicolor"]
iris["Species"] = iris["Species"].replace(["setosa", "virginica"], [0,1])
iris
```
{% include iris3.html %}

### b). Train and Test
Here, we split our data into training and testing. 80% of them are train, the rest are the test data.
```python
y = np.array(iris["Species"])
x = iris[iris.columns[0:4]]
# Using Skicit-learn to split data into training and testing sets
from sklearn.model_selection import train_test_split
# Split the data into training and testing sets
train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = 0.2)
```

### c). Create and Fit Model
Import the package we need and build of model. There is one thing we need to be notice is that Random Forest can do both classfication and Regression. Here, we want to do the classfication problems, so we import the package called "RandomForestClassifier".

```python
# Import the model we are using
from sklearn.ensemble import RandomForestClassifier
# Instantiate model with 1000 decision trees
# criterion equals to entropy is because in last post, we choose
# entropy to build our tree.
rf = RandomForestClassifier(n_estimators = 1000, criterion = "entropy")
# Train the model on training data
rf.fit(train_x, train_y)
```

### d). Accuracy
```python
predictions = rf.predict(test_x)
np.mean(predictions == test_y)
```
Our test accuracy is 100%, which is pretty much good.