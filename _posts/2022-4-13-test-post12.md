---
layout: post
title:  Random Forest
---


Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the mean or average prediction of the individual trees is returned.


I will show you one picture to see how Random Forest works in details.


![rf1.jpg]({{ site.baseurl }}/images/rf1.jpg)


## 1. Bootstrap Method
Bootstrapping is any test or metric that uses random sampling with replacement, and falls under the broader class of resampling methods. Bootstrapping assigns measures of accuracy (bias, variance, confidence intervals, prediction error, etc.) to sample estimates. This technique allows estimation of the sampling distribution of almost any statistic using random sampling methods.

So, this is the basic method that we used in Random Forest. To get higher accuracy, we use Bootstrap Method to randomly select data to create lots of trees. So, that's called Random Forest.

![rf2.png]({{ site.baseurl }}/images/rf2.png)

## 2. Bagging
Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach.

After we Bootstrap our original data, we will get a new sample. We will use this sample to create trees. After we create enough trees, we will get our final result based on these trees.

![rf3.png]({{ site.baseurl }}/images/rf3.png)

## 3. Random Forest

I will show how random forest works in Python. The "Iris" data will be used to build our model.

### a). Load Data
```python
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# import some data to play with
iris = pd.read_csv("iris.csv")
iris = iris.iloc[:,1:6]
# Only consider the Species "versicolor" and "virginica"
iris = iris[iris["Species"] != "versicolor"]
iris["Species"] = iris["Species"].replace(["setosa", "virginica"], [0,1])
iris
```
{% include iris3.html %}

### b). Train and Test
Here, we split our data into training and testing. 80% of them are train, the rest are the test data.
```python
y = np.array(iris["Species"])
x = iris[iris.columns[0:4]]
# Using Skicit-learn to split data into training and testing sets
from sklearn.model_selection import train_test_split
# Split the data into training and testing sets
train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = 0.2)
```

### c). Create and Fit Model
Import the package we need and build of model. There is one thing we need to be notice is that Random Forest can do both classfication and Regression. Here, we want to do the classfication problems, so we import the package called "RandomForestClassifier".

```python
# Import the model we are using
from sklearn.ensemble import RandomForestClassifier
# Instantiate model with 1000 decision trees
# criterion equals to entropy is because in last post, we choose
# entropy to build our tree.
rf = RandomForestClassifier(n_estimators = 1000, criterion = "entropy")
# Train the model on training data
rf.fit(train_x, train_y)
```

### d). Accuracy
```python
predictions = rf.predict(test_x)
np.mean(predictions == test_y)
```
Our test accuracy is 100%, which is pretty much good.


## 4. From Decision Tree to Random Forest

Because Random Forest is based on Decision Tree, so then, I will write some code to explain how Random Forest Works by using Decision Trees.

```python
from sklearn import tree
# create a empty list
predict = []
# this j means we want to predict the value in test_y one by one
for j in range(20):
    # we want to create 10 trees
    for i in range(0,10):
        # in every loop, we create a tree
        clf = tree.DecisionTreeClassifier()
        # randomly choose 70 observations. This step is called Bootstrap Method.
        hhh = np.random.choice(len(train_x), 70)
        # get our new train X and new train Y.
        train_y_1 = train_y[hhh]
        train_x_1 = train_x.iloc[hhh,:]
        # fit our model
        clf.fit(train_x_1, train_y_1)
        # get our test result. But this result may have 10 different output.
        # because we created 10 trees.
        predict.append(list(clf.predict(test_x[j:(j+1)]))[0])

import statistics as st
result = []
for hhh in range(0,200,10):
    # calculate the mode of every forest. Beacuse this is classfication questions
    # so, mode can help us the final prediction.
    d = st.mode(np.array(predict)[hhh:(hhh+10)])
    result.append(d)
    
np.mean(result == test_y)
```
```
1.0
```
We can see that the result is still 100%. So, this is how random forest works in details.


## 5. ROC Curve

ROC Curve is a curve that compare the false positive rate (FPR) and true positive rate ï¼ˆTPR) by changing different threshold. 

Below is how confusion matrix mean.

![rf4.png]({{ site.baseurl }}/images/rf4.png)

The formula of FPR and TPR is as below:

$$TPR = \frac{TP}{TP+FN}$$

$$FPR = \frac{FP}{FP+TN}$$

- **Threshold**

The threshold governs the choice to turn a projected probability or scores into a class label. For normalized projected probabilities in the range of 0 to 1, the threshold is set to 0.5 by default.

So, let's draw the ROC curve to see if Random Forest is a good classfier or not.

```python
from sklearn import metrics
# Import the model we are using
from sklearn.ensemble import RandomForestClassifier
# Instantiate model with 1000 decision trees
rf = RandomForestClassifier(n_estimators = 1000, criterion = "entropy")
# Train the model on training data and get our predcit probablity
probs = rf.fit(train_x, train_y).predict_proba(test_x)
preds = probs[:,1]


# compute fpr, tpr and threshold.
fpr, tpr, threshold = metrics.roc_curve(test_y, preds)
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.plot(fpr, tpr, color = "red", label = "ROC")
# draw Line of Equality
plt.plot([0,1], [0,1], color = "black", label = "Line of Equality")
plt.legend()
plt.show()
```

![rf5.png]({{ site.baseurl }}/images/rf5.png)

Here we can see the top-left of this curve is much far away from the black line, so we can consider our model is very well.


## 5. Conclusion
With that said, random forests are a strong modeling technique and much more robust than a single decision tree. They aggregate many decision trees to limit overfitting as well as error due to bias and therefore yield useful results.

So, here I want to introduce some disadvantags of Decision Trees.
1. Decision trees are prone to overfitting, especially when a tree is particularly deep.
2. Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.

Random Forest can deal with these problems very efficently. In future posts, I will introduce some more complex but funny Machine Learning Model.
