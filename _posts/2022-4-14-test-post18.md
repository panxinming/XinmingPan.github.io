---
layout: post
title:  Support Vector Machine (SVM)
---



*Support Vector Machine*, abbreviated as SVM can be used for both regression and classification tasks. But, it is widely used in **classification** objectives. The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N — the number of features) that distinctly classifies the data points.

![]({{ site.baseurl }}/images/svm1.png) ![]({{ site.baseurl }}/images/svm2.png)

To separate the two classes of data points, there are many possible hyperplanes that could be chosen. Our objective is to find a plane that has the **maximum margin**, i.e the maximum distance between data points of both classes. Maximizing the margin distance provides some reinforcement so that future data points can be classified with more confidence.



## Hyperplanes and Support Vectors

### a). Hyperplanes
![]({{ site.baseurl }}/images/svm3.png)

***Hyperplanes*** are [**decision boundaries**](https://en.wikipedia.org/wiki/Decision_boundary) that help classify the data points. Data points falling on either side of the hyperplane can be attributed to different classes. Also, the dimension of the hyperplane depends upon the number of features. If the number of input features is $$2$$, then the hyperplane is just a line. If the number of input features is $$3$$, then the hyperplane becomes a two-dimensional plane. It becomes difficult to imagine when the number of features exceeds 3.

### b). Support Vectors
![]({{ site.baseurl }}/images/svm4.jpg)

Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier. Deleting the support vectors will change the position of the hyperplane. These are the points that help us build our SVM.


## Threshold Values

In **SVM**, we take the output of the linear function and if that output is greater than $$1$$, we identify it with one class and if the output is $$-1$$, we identify is with another class. Since the threshold values are changed to $$1$$ and $$-1$$ in **SVM**, we obtain this reinforcement range of values $$[-1,1]$$ which acts as margin. We can visualize this as below.

![]({{ site.baseurl }}/images/svm4.png)

## Cost Function

Let’s write the formula for SVM’s cost function, According to one point, the cost value is as below:

$$\theta ^{T}x = \begin{bmatrix}
\theta_1\\
\theta_2\\
b\\
\end{bmatrix}
\begin{bmatrix}
x_1 & x_2 & 1\\
\end{bmatrix}$$

$$\hspace{0.6cm}= \theta_1 x_1 + \theta_2 x_2 + b$$

$$\hspace{-1.8cm} = c$$

So, $$\theta ^{T}x$$ is our predicted value.

$$\begin{equation}
Cost(h_{\theta}(x), y)=\left \{
\begin{aligned}
 max(0, 1- \theta ^{T}x) \quad , \quad y=1 \\
 max(0, 1 + \theta ^{T}x) \quad, \quad y=0
\end{aligned}
\right.
\end{equation}$$

![]({{ site.baseurl }}/images/svm.png)

The function above is the cost function to only one predicted point. The total cost function is written as:

$$J(\theta) = \sum_{i=1}^{n} y^{(i)}max(0, 1- \theta ^{T}x) + (1-y^{(i)})max(0, 1 + \theta ^{T}x) $$


## Estimated Parameters

### Part A

In previous, we know that SVM algorithm is to make the margin as big as possible. We already have the boundary. So, let's see how to estimate the parameter to get our maximum margin.

![]({{ site.baseurl }}/images/svm5.png)

From the graph, we can see that

$$y_1 - y_2 = 1  - (-1) = 2$$

$$y_1 - y_2 = (w_1 x_{11} + w_2 x_{12} +b) - (w_1 x_{21} + w_2 x_{22} +b)$$

$$\hspace{-1.2cm}= w_1(x_{11} - x_{21})+w_2(x_{12} - x_{22})$$

So,

$$w_1(x_{11} - x_{21})+w_2(x_{12} - x_{22}) = 2$$

$$\bar{w} \cdot (\bar{y_1} - \bar{y_2}) = 2$$

$$\|\bar{w}\| \cdot \|\bar{y_1} - \bar{y_2}\| \cdot cos(\theta) = 2$$

And from the graph above, we can see that, the margin $$L$$ is euqal to:

$$L = \|\bar{y_1} - \bar{y_2}\| \cdot cos(\theta)$$

Then,

$$\|\bar{w}\| \cdot L = 2$$

$$L = \frac{2}{\|\bar{w}\|}$$

Remember the **Constraint** of these paramter is:

$$y_i \cdot (\bar{w} \cdot \bar{x_i}+b) \ge 1, \quad y_i = 1\: or\: 0$$

> So, from above, Maximize $$L$$ is equivalent to minimize $$\|\bar{w}\|$$.

$$\|\bar{w}\| = \sqrt{w_1^2 + w_2^2}$$

Minimize $$\|\bar{w}\| = \sqrt{w_1^2 + w_2^2}$$ can be transformed into minimize $$\frac{\|\bar{w}\|^2}{2}$$

So, finally, we can get our parameter function as

$$f(w) = \frac{\|\bar{w}\|^2}{2}$$

We want to minimize $$f(x)$$, and we have [Constraint Function](https://en.wikipedia.org/wiki/Constraint_(mathematics))

$$g_i(w,b) = y_i \cdot (\bar{w} \cdot \bar{x_i}+b) -1 \ge 0$$

$$y_i \cdot (\bar{w} \cdot \bar{x_i}+b) -1 = p_i^2$$

So, $$g_i(w,b) = y_i \cdot (\bar{w} \cdot \bar{x_i}+b) -1- p_i^2$$

### Part B

From above, we get our parameter function $$f(w) = \frac{\|\bar{w}\|^2}{2}$$. So, in order to minimize it, we can use [Lagrangian Multiplier](https://en.wikipedia.org/wiki/Lagrange_multiplier).

Therefore, we can write our function as:

$$L(w,b,\lambda_i, p_i) = \frac{\|\bar{w}\|^2}{2} - \sum_{i=1}^{n}\lambda_i \cdot (y_i \cdot (\bar{w} \cdot \bar{x_i}+b) -1- p_i^2)$$

Then, take ***partial derivative*** of each paramter, and let them equal to 0. The solution will be our estimated parameters. However, this method is a little bit complex compare to using Python. So, let's see how to maximize $$f(w) = \frac{\|\bar{w}\|^2}{2}$$ by using python.


## Python

### 1. Create Cluster
Here, I randomly create two clusters to draw our SVM model.
```python
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.datasets import make_blobs
x, y = make_blobs(n_samples = 1000, n_features = 2, centers = 2,random_state=111)
plt.scatter(x[:,0],x[:,1])
plt.show()
```
![]({{ site.baseurl }}/images/svm6.png)

### 2. Estimated Parameter

To find the minimum value of our $$f(w) = \frac{\|\bar{w}\|^2}{2}$$, we can first define a function which contains these two parameters. Then, we can define one more function which is called *Constraint Function*, but please keep in mind. Constraint Function has four parameters which are $$w_1, w_2, b, p_i$$.

```python
# define our paramter function first.
def f(beta):
    result = 0
    for i in range(1000):
        if y[i] == 1:
            result = result+ (beta[0]**2 + beta[1]**2)/2
        elif y[i] == -1:
            result = result+ (beta[0]**2 + beta[1]**2)/2
    return result

# Then, define our Constraint Function.
def eq(beta):
    result = 0
    for i in range(1000):
        if y[i] == 1:
            result = result+ (beta[0]*x[i,0] + beta[1]*x[i,1] + beta[2] - 1 - beta[3]**2)
        elif y[i] == -1:
            result = result+ (-1*(beta[0]*x[i,0] + beta[1]*x[i,1] + beta[2]) - 1 - beta[3]**2)
    return result
```

Then, we are going to import one package that can help us find the minimum of $$f(w) = \frac{\|\bar{w}\|^2}{2}$$ very quickly.

```python
from scipy.optimize import minimize
# in order to get golbal minimum, we can try different start points.
sol = minimize(f, [2, 2, 0,0], constraints={'type': 'eq', 'fun': eq})
w1 = sol.jac[0]
w2 = sol.jac[1]
print('The estimated parameter w_1 is :', w1)
print('The estimated parameter w_2 is :', w2)
```
```
The estimated parameter w_1 is : 6.628844885221263e-05
The estimated parameter w_2 is : -0.00021558042540442647
```

<br>

After we get our estimated parameters, let's see how to draw the boundary and how SVM model in figure.
```python
b1 = 10
b2 = 10
for i in range(1000):
    if y[i] == 1:
        if x[i,1]-(-w1/w2)*x[i,0] < b1:
            b1 = x[i,1]-(-w1/w2)*x[i,0]
    elif y[i] == 0:
        if (-w1/w2)*x[i,0] - x[i,1] < b2:
            b2 = (-w1/w2)*x[i,0] - x[i,1]
print(b1, b2)
```
```
2.320668405784904 4.322118960759816
```

<br>

So, we can draw the boundary as:
```python
x1 = np.linspace(-5,5,100)
x2 = (-w1/w2)*x1 + b1
x3 = (-w1/w2)*x1 - b2

plt.scatter(x[:,0],x[:,1])
plt.plot(x1,x2, label = "Boundary 1")
plt.plot(x1,x3, label = "Boundary 2")
plt.legend()
plt.show()
```
![]({{ site.baseurl }}/images/svm7.png)

At last, let's draw our optimal hyperplane by hand.
```python
x1 = np.linspace(-5,5,100)
x2 = (-w1/w2)*x1 + b1
x3 = (-w1/w2)*x1 - b2
x4 = (-w1/w2)*x1 + (b1-b2)/2
plt.scatter(x[:,0],x[:,1])
plt.plot(x1,x2,'--r', label = "Boundary 1")
plt.plot(x1,x3,'--g', label = "Boundary 2")
plt.plot(x1,x4,'-m', label = "Optimal Hyperplane")
plt.legend()
plt.show()
```

![]({{ site.baseurl }}/images/svm8.png)

### 3. SVM in Sklearn

The support vector machines in scikit-learn support both dense (`numpy.ndarray` and convertible to that by `numpy.asarray`) and sparse (any `scipy.sparse`) sample vectors as input.

```python
from sklearn import svm
clf = svm.SVC()
clf.fit(x, y)


plt.scatter(x[:,0],x[:,1])
plt.scatter(clf.support_vectors_[:,0], clf.support_vectors_[:,1], 
            label = "Support Vector")
plt.legend()
plt.show()
```
![]({{ site.baseurl }}/images/svm9.png)


Then, Let's draw the decision region to see if it's perfect compared to our calculation.
```python
def make_meshgrid(x, y, h=.02):
    x_min, x_max = x.min() - 1, x.max() + 1
    y_min, y_max = y.min() - 1, y.max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    return xx, yy

def plot_contours(clf, xx, yy, **params):
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    out = plt.contourf(xx, yy, Z, **params)
    return out

xx, yy = make_meshgrid(x[:, 0], x[:, 1])
plt.scatter(x[:,0],x[:,1], c=y, cmap=plt.cm.coolwarm, s=20)
plot_contours(clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.3)
plt.show()
```
![]({{ site.baseurl }}/images/svm10.png)


Finally, we can see the decision region is almost same compared to our calculation. So, this is the way how to write SVM model by hand.


## Conclusion

The advantages of support vector machines are:
- Effective in high dimensional spaces.
- Still effective in cases where number of dimensions is greater than the number of samples.
- Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.

The disadvantages of support vector machines include:
- If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.
- SVMs do not directly provide probability estimates

<br>


SVMs are one of the most robust prediction methods, being based on statistical learning. SVM is very useful and powerful [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning) models with associated learning algorithms that analyze data for classification and regression analysis. In next post, I will introduce more complex machine learning models.


