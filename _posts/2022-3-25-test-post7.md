---
layout: post
title: Logistic Regression
---

Here I want to talk about one Machine Learning Model, Logistic Regression.\
Logistic regression models the probabilities for classification problems with two possible outcomes. It’s an extension of the linear regression model for classification problems.


***So, Here I want to ask one question.***
> Why not choose Linear Regression for Classification?

> **Answer:** The linear regression model can work well for regression, but fails for classification. Why is that? In case of two classes, you could label one of the classes with 0 and the other with 1 and use linear regression. Technically it works and most linear model programs will spit out weights for you. But there are a few problems with this approach: A linear model does not output probabilities, but it treats the classes as numbers (0 and 1) and fits the best hyperplane (for a single feature, it is a line) that minimizes the distances between the points and the hyperplane [(Least Square Method)](https://en.wikipedia.org/wiki/Least_squares). So it simply interpolates between the points, and you cannot interpret it as probabilities.


Now, let's compare Linear Regression and Logistic Regression.

Here, I load the **iris** data as our example.

## 1. Linear Regression

Load our data first in Python.
```python
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# import some data to play with
iris = pd.read_csv("iris.csv")
iris
```

{% include iris.html %}

After doing some data cleaning, then we draw the scatter plot.
```python
# Only consider the Species "versicolor" and "virginica"
iris = iris[iris["Species"] != "versicolor"]
iris["Species"] = iris["Species"].replace(["setosa", "virginica"], [0,1])

# Create Variables X and Y
x = iris["Sepal.Length"]
y = iris["Species"]
x = np.array(x)
y = np.array(y)
# Create the scatter plot
plt.scatter(x,y)
plt.xlabel("Sepal.Length")
plt.ylabel("Species")
plt.show()
```

![iris.lm.1.png]({{ site.baseurl }}/images/iris.lm.1.png)


Then, we are going to draw the linear regression line. First, let get our linear equation function.
```python
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(x.reshape(-1,1), y)
beta_0 = model.intercept_
beta_1 = model.coef_[0]
print('The intercept is :', beta_0)
print('The slope is :', beta_1)
```
```
The intercept is : -2.091541566490447
The slope is : 0.4470487435726146
```

After we get our estimated parameters, then we are going to draw the line.

```python
# Create the scatter plot
plt.scatter(x,y)
# draw the line
plt.plot(x,beta_0+beta_1*x)
plt.xlabel("Sepal.Length")
plt.ylabel("Probability")
plt.show()
```


![iris.lm.2.png]({{ site.baseurl }}/images/iris.lm.2.png)

> Here, we can see the **Linear Regression** line doesn't fit the plot very much. So, now let's see if **Logistic Regression** can do better.


## 2. Logistic Regression

Before we draw the Logistic Regression line, I want to introduce some **mathematical** knowledge of Logistic Regression.

Logistic Regression gives the probability of an event occurring, not just the predicted classification.

***Odds*** means a ratio between the probability of the event you want to happen (y = 1) and the probability of the event you don't want to happen (y = 0).

$$odds = \frac{P(y = 1)}{1-P(y = 1)}$$

### A. Odds Prediction

$$\frac{P}{1-P} = e^{\beta_0 + \beta_1 x_1 + ... + \beta_n x_n}$$

it's equivalent to:

$$ln(\frac{P}{1-P}) = \beta_0 + \beta_1 x_1 + ... + \beta_n x_n$$


### B. Sigmoid Function

We can simply consider **Logistic Regression = Linear Regression + Sigmoid Function**

Sigmoid Function is $$sig(t) = \frac{1}{1+e^{-t}}$$, Let's see how Sigmoid Function looks like.
```python
import numpy as np
import math
t = np.arange(-10,10,0.1) 
e = math.exp(1)

def y(t):
    return 1/(1+e**(-t))

plt.plot(t,y(t))
plt.title("Plot of Sigmoid Function")
plt.show()
```
![lm3.png]({{ site.baseurl }}/images/lm3.png)


And the function of **Linear Regression** is $$y_n = \beta_0 + \beta_1 x_{n1} + ... + \beta_n x_{nn}$$ . 

We can also write it as $$Y = \beta X$$ .

It's also equivalent to:
$$\begin{bmatrix}
y_1\\
y_2\\
..\\
y_n
\end{bmatrix} = \begin{bmatrix}
1 & x_{11} & x_{12} & .. & x_{1n}\\
1 & x_{21} & x_{22} & .. & x_{2n}\\
 .. & .. & .. & .. & ..\\
1 & x_{n1} & x_{n2} & .. & x_{nn}\\
\end{bmatrix}
\begin{bmatrix}
\beta_0\\
\beta_1\\
..\\
\beta_n
\end{bmatrix}$$


So, the function of **Logistic Regression** is:

$$p(x) = \frac{1}{1+e^{-\beta X}}$$

Here, it's it's equivalent to:

$$p(x) = \frac{1}{1+e^{-(\beta_0 + \beta_1 x)}}$$

### C. Draw Logistic Regression

Because we know **Logistic Regression** is a combination of **Linear Regression** and **Sigmoid Function**, but the estimated parameters $$\beta_0$$ and $$\beta_1$$ are not the same as linear regression. 

<br />

From linear regression we know we can get our estimated parameters by using ***Least Square***, ***Pearson’s Correlation Coefficient*** etc. Here I want to introduce one way to get our estimated parameters of Logistic Regression which is [Maximum Likelihood Estimate](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation)

> Maximum Likelihood Estimate


The formula of MLE is:

$$L(\theta |x) = \prod_{i=1}^{n}P(\theta |x_i)$$

$$\hspace{5.7cm} =P(\theta |x_1)\cdot P(\theta |x_2) ... P(\theta |x_n)$$

where $$\theta$$ is our estimated parameter.

 <br />

 Because the outcome $$y$$ is 0 or 1, so we can treat it as **Bernoulli Distribution**. It can be written as:

 $$P_{y_i}(x) \begin{equation}
=\left\{
\begin{aligned}
p(x) \quad , \quad y=1 \\
1-p(x) \quad , \quad y=0
\end{aligned}
\right.
\end{equation}
$$ 

$$\hspace{3.1cm} \begin{equation}
=\left\{
\begin{aligned}
\frac{1}{1+e^{-(\beta_0 + \beta_1 x)}} \quad , \quad y=1 \\
\frac{1}{1+e^{\beta_0 + \beta_1 x}} \quad , \quad y=0
\end{aligned}
\right.
\end{equation}$$


So, the Probability Mass Function can be written as:

$$P_{y_i}(x) = P(x)^y \cdot (1-p(x))^{1-y}$$

$$\hspace{4.7cm}=(\frac{1}{1+e^{-(\beta_0 + \beta_1 x)}})^y (\frac{1}{1+e^{\beta_0 + \beta_1 x}})^{1-y}$$

$$\hspace{3cm}=\frac{e^{y(\beta_0 + \beta_1 x)}}{1+e^{\beta_0 + \beta_1 x}}, \quad \{ y \in (0,1)\}$$

So, here we have our Likelihood Function as:

$$L_{n}(y_1,y_2,...,y_n | \theta) = \prod_{i=1}^{n} p_{\theta}(y_i)$$

$$L_{n}(y_1,y_2,...,y_n | \beta_0, \beta_1) = \prod_{i=1}^{n} \frac{e^{y_{i}(\beta_0 + \beta_1 x)}}{1+e^{\beta_0 + \beta_1 x}}$$

Then, we are going to find the estimated parameters to make our Likelihood Function maximum. In mathematical way, we need to take log both sides and take derivative then. But here, we can use Python to get our Maximum Likelihood Function and estimated parameters.


```python
def L(beta):
    '''
    This function is the negative Likelihood Function. Because we want to find 
    the maximum of Likelihood Function, so it euqals find the minimum of the negative
    Likelihood Function.
    
    This function has one argument.
    beta: beta is the parameter we want to estimate, it's a list which is combined with 
         beta_0 and beta_1
    '''
    result = 1
    # because our sample size is 100.
    for i in range(100):
        # if y = 1, get our Likelihood function as below
        if y[i] == 1:
            result = result* ( np.exp(beta[0] + beta[1]*x[i])/(1+np.exp(beta[0] + beta[1]*x[i])) )
        # if y = 0, get our Likelihood function as below
        else:
            result = result* 1/(1+np.exp(beta[0] + beta[1]*x[i]))
    return -result
```

We get our Likelihood Function written by Python. Then, I am going to load one package that help us find the mimimum of one function very fast and efficient. Here, we let our function equal to the negative of Likelihood Function, so finding the minimum of this function is equal to find the maximum of Likelihood Function.
```python
from scipy.optimize import fmin
beta_0 = fmin(L,[0,0])[0]
beta_1 = fmin(L,[0,0])[1]
print('The estimated parameter beta_0 is :', beta_0)
print('The estimated parameter beta_1 is :', beta_1)
```
```
The estimated parameter beta_0 is : -36.68200193793513
The estimated parameter beta_1 is : 6.672914800010198
```

So, we can get our final Logistic Regression Function as:

$$p(x) = \frac{1}{1+e^{36.68 - 6.67 x}}$$


Finally, Let's draw the Logistic Regression to see if it's better than Linear Regression or not.
```python
def my_logis(x):
    '''
    This is the function of our Logistic Regression.
    '''
    prop = 1/(1+np.exp(-(beta_1*x+beta_0)))
    return prop


# draw the scatter plot of x and y.
x = np.sort(x)
y = np.sort(y)
plt.scatter(x,y)
plt.xlabel("Sepal.Length")
plt.ylabel("Probability")
# get our predicted y
y_hat = my_logis(x)
# draw the logistic regression line
plt.plot(x,y_hat, color = "orange")
plt.show()
```
![lm12.png]({{ site.baseurl }}/images/lm12.png)

It's much better than Linear Regression and it fits our data very well.

### D. Loss Function of Logistic Function

We can calculate the **Loss Function** to test if our **Logistic Regression** is good or not.


> Let's see how this Loss Function works

1. If our outcome is $$y = 1$$, then $$p(x)$$ is our predicted outcome. If $$p(x)$$ is close to 1, then the loss will be small. So, we choose $$C = -log(p(x))$$ as our loss function. The figure is show as below.


```python
x = np.arange(0,1,0.01) 

def y(x):
    return -np.log(x)

# loss function when y = 1.
plt.plot(x,y(x))
plt.title("y = 1")
plt.xlabel("p(x)")
plt.ylabel("cost value")
plt.show()
```

![lm4.png]({{ site.baseurl }}/images/lm4.png)

2. When $$y=0$$, so the loss value shold be large when $$p(x)$$ is close to 1. So, the figure is show as below.

```python
x = np.arange(0,1,0.01) 

def y(x):
    return -np.log(1-x)

plt.plot(x,y(x))
plt.title("y = 0")
plt.xlabel("p(x)")
plt.ylabel("cost value")
plt.show()
```
![lm5.png]({{ site.baseurl }}/images/lm5.png)


So, overall. The Loss Function of Logistic Regression can be written as:

$$L(p_i(x), y) = -y\cdot log(p_i(x)) - (1-y)\cdot log(1-p_i(x)), y \in \{0,1\}$$

$$\begin{equation}
L(p_i(x))=\left \{
\begin{aligned}
 -log(p_i(x)) \quad , \quad y=1 \\
-log(1-p_i(x)) \quad , \quad y=0
\end{aligned}
\right.
\end{equation}$$


$$L = \frac{1}{n} \sum_{i=1}^{n} L(p_i(x))$$


Because from graph, we know that Logistic Regression fits our data very well. So, the loss value must be very small. If loss value approaches 0, that means our model is very well. Otherwise, it's bad.


Let's define our loss funtion in Python.
```python
def loss_function(x):
    result = 0
    # because our sample size is 100.
    for i in range(100):
        # if y = 1, get our loss function.
        if y[i] == 1:
            result += -np.log(my_logis(x[i]))
        # if y = 0, get our loss function.
        else:
            result += -np.log(1-my_logis(x[i]))
    # divide by the sample size
    return result/100


loss_function(x)
```
```
0.06318772326556397
```

The loss value is very small, so that means our model is very well.

### E. Test Logistic Regression

We try to use the data above to test if Logistic Regression can do classification well or not. And try to compare it with linear regression.

```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
cols = [0,1,2,3]
x = iris[iris.columns[cols]]
# split our data into train and test
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)
```

Then, we are going to fit the logistic regression.
```python
# create logistic model
logreg = LogisticRegression()
# fit the model by using train data
logreg.fit(x_train, y_train)
# predict the outcome by using test data
y_pred = logreg.predict(x_test)
# test the accuracy.
logreg.score(x_test, y_test)
```
```
0.9
```

The accuracy is **90%** which is extremely excellent.

Let's draw the confusion matrix.
```python
from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_test, y_pred)
print(confusion_matrix)
```
```
[[ 8  2]
 [ 0 10]]
```

### F. ROC Curve
The Receiver Operating Characteristic (ROC) curve is another common tool used with binary classifiers. The red line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner). I will draw this ROC curve step by step.


Let's write a function that can help us calculate the false positive rate and true positive rate by changing the threshold C.
```python
def rates(x,y,c):
    '''
    This function can help us calculate the false positive rate 
    and true positive rate by changing the threshold C.
    '''
    # get our predicted y
    y_predict = my_logis(x)
    # if predicted y bigger than C, then we consider those value are 1.
    correct = y[y_predict > c]
    
    # false positive rate means those values are not 1 divided by the true value y equal to 0. 
    fpr = len(correct[correct == 0])/len(y[y==0])
    # true positive rate means those values are 1 divided by the true value y equal to 1.
    tpr = len(correct[correct == 1])/len(y[y==1])
    
    return(fpr,tpr)
```


Then, after we get our false positive rate and true positive rate by different threshold. We can draw the ROC curve.


```python
fpr = []
tpr = []
# c: threshold c between 0 and 1
c = np.linspace(0,1,100)
for i in c:
    rate = rates(x,y,i)
    # compute false positive rate
    fpr.append(rate[0])
    # compute true positive rate
    tpr.append(rate[1])
# draw ROC
plt.plot(fpr, tpr, color = "red", label = "ROC")
# draw Line of Equality
plt.plot([0,1], [0,1], color = "black", label = "Line of Equality")
plt.legend()
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.show()
```
![lm6.png]({{ site.baseurl }}/images/lm6.png)

Here we can see the top-left of this curve is much far away from the black line, so we can consider our model is very well.


## 3. Conclusion

Logistic Regression is a very important model in Machine Learning. It's used for binary classification. However, now we have muti-variate logistic regression which allow us to do muti-variate classification. Anyway, This is how Logistic Regression works. In next serval posts, I will discuss more Machine Learning Models.