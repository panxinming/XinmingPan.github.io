---
layout: post
title: Logistic Regression
---

Here I want to talk about one Machine Learning Model, Logistic Regression.\
Logistic regression models the probabilities for classification problems with two possible outcomes. Itâ€™s an extension of the linear regression model for classification problems.


***So, Here I want to ask one question.***
> Why not choose Linear Regression for Classification?

> **Answer:** The linear regression model can work well for regression, but fails for classification. Why is that? In case of two classes, you could label one of the classes with 0 and the other with 1 and use linear regression. Technically it works and most linear model programs will spit out weights for you. But there are a few problems with this approach: A linear model does not output probabilities, but it treats the classes as numbers (0 and 1) and fits the best hyperplane (for a single feature, it is a line) that minimizes the distances between the points and the hyperplane [(Least Square Method)](https://en.wikipedia.org/wiki/Least_squares). So it simply interpolates between the points, and you cannot interpret it as probabilities.


Now, let's compare Linear Regression and Logistic Regression.

Here, I load the **iris** data as our example.

## 1. Linear Regression

Load our data first in Python.
```python
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sb

# import some data to play with
iris = pd.read_csv("iris.csv")
iris
```

{% include iris.html %}

After doing some data cleaning, then we draw the scatter plot.
```python
# Only consider the Species "versicolor" and "virginica"
iris = iris[iris["Species"] != "setosa"]
iris["Species"] = iris["Species"].replace(["versicolor", "virginica"], [0,1])

# Create Variables X and Y
x = iris["Sepal.Length"]
y = iris["Species"]
# Create the scatter plot
sb.scatterplot(x,y)
```

![iris.lm.1.png]({{ site.baseurl }}/images/iris.lm.1.png)


Then, we are going to draw the linear regression line.
```python
sb.regplot(x,y, ci = None)
```

![iris.lm.2.png]({{ site.baseurl }}/images/iris.lm.2.png)

> Here, we can see the **Linear Regression** line doesn't fit the plot very much. So, now let's see if **Logistic Regression** can do better.


## 2. Logistic Regression

Before we draw the Logistic Regression line, I want to introduce some **mathematical** knowledge of Logistic Regression.

Logistic Regression gives the probability of an event occurring, not just the predicted classification.

***Odds*** means a ratio between the probability of the event you want to happen (y = 1) and the probability of the event you don't want to happen (y = 0).

$$odds = \frac{P(y = 1)}{1-P(y = 1)}$$

### A. Odds Prediction

$$\frac{P(y = 1)}{1-P(y = 1)} = e^{\beta_0 + \beta_1 x_1 + ... + \beta_n x_n}$$

it's equivalent to:

$$ln(\frac{P(y = 1)}{1-P(y = 1)}) = \beta_0 + \beta_1 x_1 + ... + \beta_n x_n$$


### B. Sigmoid Function

We can simply consider **Logistic Regression = Linear Regression + Sigmoid Function**

Sigmoid Function is $$sig(t) = \frac{1}{1+e^{-t}}$$, Let's see how Sigmoid Function looks like.
```python
import numpy as np
import math
x = np.arange(-10,10,0.1) 
e = math.exp(1)

def y(x):
    return 1/(1+e**(-x))

plt.plot(x,y(x))
plt.title("Plot of Sigmoid Function")
plt.show()
```
![lm3.png]({{ site.baseurl }}/images/lm3.png)


And the function of **Linear Regression** is $$y_n = \beta_0 + \beta_1 x_{n1} + ... + \beta_n x_{nn}$$ . 

We can also write it as $$Y = \beta X$$ .

It's also equivalent to:
$$\begin{bmatrix}
y_1\\
y_2\\
..\\
y_n
\end{bmatrix} = \begin{bmatrix}
1 & x_{11} & x_{12} & .. & x_{1n}\\
1 & x_{21} & x_{22} & .. & x_{2n}\\
 .. & .. & .. & .. & ..\\
1 & x_{n1} & x_{n2} & .. & x_{nn}\\
\end{bmatrix}
\begin{bmatrix}
\beta_0\\
\beta_1\\
..\\
\beta_n
\end{bmatrix}$$


So, the function of **Logistic Regression** is:

$$p(x) = \frac{1}{1+e^{-\beta X}}$$


### C. Loss Function of Logistic Function

[Gradient Descent Algorithm](https://en.wikipedia.org/wiki/Gradient_descent) (You can find this method in my post) is an useful method to calcuate the parameter of Linear Regression.

After we get the estimate of parameter which is the $$\beta$$, we can apply these parameters into Logistic Regression. Then, we can calculate the **Loss Function** to test if our **Logistic Regression** is good or not.


> Let's see how this Loss Function works

1. If our true outcome is $$y = 1$$, then $$p(x)$$ is our predicted outcome. If $$p(x)$$ is close to 1, then the loss will be small. So, we choose $$C = -log(p(x))$$ as our loss function. The figure is show as below.


```python
x = np.arange(0,1,0.01) 

def y(x):
    return -np.log(x)

# loss function when y = 1.
plt.plot(x,y(x))
plt.title("y = 1")
plt.xlabel("p(x)")
plt.ylabel("cost value")
plt.show()
```

![lm4.png]({{ site.baseurl }}/images/lm4.png)

2. When $$y=0$$, so the loss value shold be large when $$p(x)$$ is close to 1. So, the figure is show as below.

```python
x = np.arange(0,1,0.01) 

def y(x):
    return -np.log(1-x)

plt.plot(x,y(x))
plt.title("y = 0")
plt.xlabel("p(x)")
plt.ylabel("cost value")
plt.show()
```
![lm5.png]({{ site.baseurl }}/images/lm5.png)


So, overall. The Loss Function of Logistic Regression can be written as:

$$L(p(x), y) = -y\cdot log(p(x)) - (1-y)\cdot log(p(x)), y \in \{0,1\}$$


### D. Test Logistic Regression

We try to use the data above to test if Logistic Regression can do classification well or not. And try to compare it with linear regression.

```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
cols = [0,1,2,3]
x = iris[iris.columns[cols]]
# split our data into train and test
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)
```

Then, we are going to fit the logistic regression.
```python
# create logistic model
logreg = LogisticRegression()
# fit the model by using train data
logreg.fit(x_train, y_train)
# predict the outcome by using test data
y_pred = logreg.predict(x_test)
# test the accuracy.
logreg.score(x_test, y_test)
```
```
0.9
```

The accuracy is **90%** which is extremely excellent.

Let's draw the confusion matrix.
```python
from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_test, y_pred)
print(confusion_matrix)
```
```
[[ 8  2]
 [ 0 10]]
```

### E. ROC Curve
The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers. The dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner).

```python
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
logit_roc_auc = roc_auc_score(y_test, logreg.predict(x_test))
fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(x_test)[:,1])
plt.figure()
plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()
```

![lm6.png]({{ site.baseurl }}/images/lm6.png)


## 3. Conclusion

Logistic Regression is a very important model in Machine Learning. It's used for binary classification. However, now we have muti-variate logistic regression which allow us to do muti-variate classification. Anyway, This is how Logistic Regression works. In next serval posts, I will discuss more Machine Learning Models.